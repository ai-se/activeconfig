There are many ways to build defect predictors
such as  CART~\cite{brieman00}, Random Forest~\cite{breiman84}, 
and WHERE~\cite{menzies2013local}.
For this study, we use the CART and Random Forest  from 
SciKitLearn~\cite{scikit-learn} (and
WHERE is available from
github.com/ai-se/where).
 We use  these algorithms, for the following reasons.
CART and Random Forest were mentioned in
a recent IEEE TSE paper by Lessmann et al.~\cite{lessmann2008benchmarking} that compared 21  
learners for  defect prediction.
That study ranked  CART  worst  and Random Forest as best.
In a demonstration of the impact of tuning,
this paper shows  we can {\em reverse} the conclusions of  Lessmann et al. such that CART
performs just as well as
 Random Forest.
This
 paper also presents results with WHERE-based learner since, as shown below,
it offers an interesting case study on the benefits of tuning.
  

\subsection{Learners and Their Tunings}


Our learners use the tuning parameters of \fig{parameters}. This section describes those parameters.

CART, Random Forest, and WHERE-based learner are all  tree learners that divide a data set, then recurse
on each split.
All these learners
generate numeric predictions which are converted
into binary ``yes/no'' decisions via \eq{yesno}. Hence, they all use the {\em threshold} value $T$ discussed in \tion{eg}.

The splitting process is controlled by numerous tuning parameters.
If data contains more than {\em min\_sample\_split}, then a split is attempted.
On the other hand, if a split contains no more than {\em min\_samples\_leaf}, then recursion stops. For CART and Random Forest use a 
user-supplied constant for this parameter while
WHERE-based learner firstly computes this parameter $m$={\em min\_samples\_leaf} from the size of the data
sets via  $m=\mathit{n_samples}^\mathit{min\_size}$ to build an initial  clustering tree.
Note that WHERE builds {\em two} trees: the initial clustering tree (to find similar sets of data)
then a final decision tree (to learn rules that predict for each similar cluster)\footnote{A
frequently asked question is why does WHERE build two trees--
would not   a single tree suffice? The answer is, as shown below,  tuned WHERE's twin-tree approach 
generates very precise predictors.}.
The tuning parameter  {\em min\_sample\_ split } controls the construction of the
final decision tree (so, for WHERE-based learner,
{\em min\_size} and {\em min\_sample\_split} are the parameters to be tuned).

These learners use different techniques to explore the splits:
\bi
\item
CART finds the attributes whose ranges contain rows with least variance in the number
of defects\footnote{If an attribute ranges $r_i$ is found in 
$n_i$ rows each with a  defect count variance of $v_i$, then CART seeks the attributes
whose ranges minimizes $\sum_i \left(\sqrt{v_i}\times n_i/(\sum_i n_i)\right)$.}.
\item
Random Forest    divides data like CART then  builds $F>1$  trees,
each time using some random subset of
the attributes. 
\item
When building the initial cluster tree, WHERE projects the data on to a dimension it synthesizes from the raw data using
a process analogous to principle component analysis(PCA)\footnote{
PCA  synthesises  new
attributes $e_i, e_2,...$
that extends across the dimension of greatest  variance in the data  with attributes $d$.  
This process  combines
redundant  variables into a smaller set of variables  (so $e \ll d$) since those
redundancies become (approximately) parallel lines
in $e$ space. For all such redundancies \mbox{$i,j \in d$}, we 
can ignore $j$ 
since effects that change over $j$ also
change in the same way over $i$.
PCA is also useful for skipping over noisy variables from $d$-- these
variables are effectively ignored since    they  do not contribute to the variance in the data.}.
WHERE  divides  at the median point of that projection.
On recursion,
this generates the initial clustering tree, the leaves of which are clusters of  very similar examples. After that, when building 
the final decision tree, WHERE pretends its clusters are ``classes'', then 
asks the InfoGain of the
Fayyad-Irani discretizer~\cite{FayIra93Multi}, to rank the attriubutes, where {\em infoPrune} is used.
WHERE's final decision tree generator then ignores everything except the top   {\em infoPrune} percent of the sorted
attributes.
\ei
Some tuning parameters are learner specific:
\bi
\item
{\em Max\_feature} is used by
CART and Random Forest to select the number of attributes
used to build one tree.
CART's default is to use all the attributes while 
Random Forest usually selects the square root of the number
of attributes.
\item
  {\em Max\_leaf\_nodes} is the upper bound on leaf notes generated in a 
  Random Forest.
\item {\em Max\_depth} is the upper bound on the depth of the CART tree.  
 \item
WHERE's initial clustering
tree generation will always split up to {\em depthMin} number of branches.
After that, WHERE will only split data if the mean performance scores of the two halves
is ``trivially small'' (where ``trivially small'' is set by the   {\em wriggle} parameter). 
\item
WHERE's   {\em tree\_prune} setting controls how   
WHERE prunes back superflous parts of the final decision tree. 
If a decision sub-tree and its parent have the same 
majority cluster
(one that occurs most frequently), then if {\em tree\_prune} is enabled, we prune that decision sub-tree.
\ei





\subsection{Tuning Algorithms}


 \subsubsection{Parametric Tuning Algorithms}
The  goal of this paper is to adjust the tuning parameters of \fig{parameters}
in order to   optimize (improve) some particular performance scores
generated by a particular learner being applied to  a particular data set.
For this task, we do not use traditional parametric numeric optimizer  
such as  gradient descent optimizers~\cite{saltelli00} that require models comprised of
differential functions (i.e. functions of real-valued variables whose derivative exists at each point in its domain).
This is impractical  for  our learners since their internal states are   not a smoothly differential continuous function.
Rather, learners being tuned  contain many regions with many different properties (tuning options can
drive the learner into very different modes with very different performance properties).


 \subsubsection{Non-Parametric Tuning Algorithms}
 
Non-parametric  optimizers   make no assumption
about the internal structure of the model.
There are many such optimizers:
\bi
\item
Simulated annealing~\cite{fea02a,me07f};
\item
A wide range of genetic algorithms~\cite{goldberg79} augmented by
techniques such as differential evolution~\cite{storn1997differential}, tabu search and scatter search~\cite{Glover1986563,Beausoleil2006426,Molina05sspmo:a,4455350};
\item
Particle swarm optimization~\cite{pan08}; 
\item 
Numerous ``decomposition approaches'' that uses
    heuristics to decompose the total space into   small problems,   then applies a
    simpler optimizer to each region including \mbox{$\varepsilon$-domination}~\cite{deb05} and MOEA/D ~\cite{zhang07};
    \item 
    Response surface methods~\cite{krall15,Zuluaga:13};
    \item
    And many others as well.
    \ei
From all the above methods, how do we select which optimizers to apply to tuning data miners?
Cohen~\cite{cohen95} advises comparing new 
 methods against the simplest possible alternative. 
Holte~\cite{holte93} recommends using very simple  learners
 as a
kind of ``scout'' for a  preliminary analysis of a data
set (to check if that data really requires a more
complex analysis).

To find our ``scout'',  we used engineering judgement to sort  the above algorithms from simplest to most complex.
Of the optimizers listed above, the simplest are are SA  and 
differential evolution (each can be coded in less than a page of some high-level scripting language). Our reading of the current literature is that there are more  advocates for
differential evolution than
  SA:
  \bi
  \item
  When the MOEA/D community requires a secondary optimizer, they often use  differential evolution~\cite{zhang07,5583335}.
  \item
 Vesterstrom and Thomsen~\cite{Vesterstrom04} found DE to be competitive with 
   particle swarm optimization and other GAs.
   \ei
DEs have been applied before for   parameter tuning (e.g. see~\cite{omran2005differential, chiha2012tuning}) but this is the first time they have been applied to
optimize defect prediction from static code attributes.  


 
 

 