\documentclass{sig-alternative}
% \documentclass[conference]{IEEEtran}
\usepackage{color}
\usepackage{listings}
\usepackage{graphicx} 
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{cite}
\usepackage{paralist}
\usepackage[table,xcdraw]{xcolor}
\usepackage{siunitx}
\usepackage{rotating}
\usepackage{eqparbox}
\usepackage{graphics}
\usepackage{colortbl} 
\usepackage{multirow}
\usepackage{times}
\usepackage{balance}
\usepackage{picture}

\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
\renewcommand{\footnotesize}{\scriptsize}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[table]{xcolor}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\lstset{
    language=Python,
    basicstyle=\ttfamily\fontsize{2.7mm}{0.8em}\selectfont,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=l,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\bfseries,
    emph={furthest,gale,better,improved,where,fastmap,split,project,mutate,mutate1}, emphstyle=\bfseries\color{blue},
    stringstyle=\color{green!50!black},
    commentstyle=\color{gray}\itshape,
    numbers=left,
    captionpos=t,
    escapeinside={\%*}{*)}
}


%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}
%\newcommand{\rone}{\crule{1mm}{1.95mm}}
%\newcommand{\rtwo}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
%\newcommand{\rthree}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
%\newcommand{\rfour}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}} 
%\newcommand{\rfive}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
\newcommand{\quart}[3]{\begin{picture}(100,6)%1
{\color{black}\put(#3,3){\circle*{4}}\put(#1,3){\line(1,0){#2}}}\end{picture}}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}
% \newcommand{\rone}{}
% \newcommand{\rtwo}{}
% \newcommand{\rthree}{}
% \newcommand{\rfour}{} 
% \newcommand{\rfive}{}
\newcommand{\wei}[1]{\textcolor{red}{Wei: #1}} 
\newcommand{\Menzies}[1]{\textcolor{red}{Dr.Menzies: #1}} 
%% timm tricks
\newcommand{\bi}{\begin{itemize}}%[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table ~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}

%% space saving measures

%\usepackage[shortlabels]{enumitem}  
\usepackage{url}
% \def\baselinestretch{1}


% \setlist{nosep}
%  \usepackage[font={small}]{caption, subfig}
% \setlength{\abovecaptionskip}{1ex}
%  \setlength{\belowcaptionskip}{1ex}

%  \setlength{\floatsep}{1ex}
%  \setlength{\textfloatsep}{1ex}
%  \newcommand{\subparagraph}{}

% \usepackage[compact,small]{titlesec}
% \DeclareMathSizes{7}{7}{7}{7} 
% \setlength{\columnsep}{7mm}

\begin{document}
% \conferenceinfo{FSE}{'15 Bergamo, Italy}
\title{Faster Discovery  of Faster Programs \\from Software Product Lines} 
\numberofauthors{2}
\author{
        \alignauthor Vivek Nair, Tim Menzies, Xipeng Shen 
        \affaddr{Computer Science, North Carolina State University, Raleigh, USA}
        \email{vivekaxl, tim.menzies, xipengshen@gmail.com}
    \and  
        \alignauthor Norbert Siegmund, Sven Apel \\
        \affaddr{Computer Science, University of Passau, Germany}\\
        \email{norbert.siegmund, apel@uni-passau.de}
       }

\maketitle 
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}
Prior work on predicting runtimes from products generated from
software product lines suffered from either (a)~needing far to many samples
of running programs or (b)~large variances in their predictions.
Both these problems can be avoided using the ZIP
spectral clusterer. 
ZIP  uses the spectrum (eigenvalues) of a distance matrix
between configurations to perform dimensionality reduction. Within that
reduced space, many closely associated configurations can be studied
via the execution of just a few samples; e.g. for the programs studied
here, a few dozen samples yielded accurate predictors (less than 10\% error)
that were very stable (standard deviations usually less than 3\%).  
Also, when compared to a prior state of the art result passed on
pairwise sampling, ZIP
 required  
25\% to 50\% less samples 
and its predictions are   four to sixteen times more accurate (measured in terms of mean or
standard deviations  of the errors).  Further, the predictive model generated by
WHERE can be used by optimizers to infer new configurations that closely
approach the minimum runtimes of all possible configurations for a software system.
\end{abstract}

% A category with the (minimum) three required fields
\vspace{1mm}
\noindent
{\bf Categories/Subject Descriptors:} 
D.2 [Software Engineering] ;
I.2.6 [Artificial Intelligence]: Induction

 
\vspace{1mm}
\noindent

{\bf Keywords:} Performance prediction, Active Learning, 
Multi- Objective Optimization,
Search-based Software Engineering,Sampling, Machine Learning.

\pagenumbering{arabic} %XXX delete before submission
 
\begin{comment}

note that our analysis is divided into (1) a clustering method on the independent variables (settings to the Makefiles) then (2) an execution of a subset of the clustered data

where we win is that we do (2) very few times (recall that this is time consuming bit)  . you cans ee where we win in fig5, fig6

\end{comment}
 
\section{Introduction}
 
 Are software  product line models too abstract for concrete reasoning?
Or is it possible to use these product lines to predict specific properties 
of programs generated from the product line? 

Software product line models are highly level descriptions of the commonalities and variabilities   across related software systems. They can be used to derive a set of approaches for software implementation. 
They have received much attention in the literature.
For example, the 1990 Kang feature models article (that describes one notation for product lines) has over 4600 citations~\cite{kang1990feature} and are the basis for 
much recent work~\cite{lopez15,harman14,sayyad13a,sayyad13b}).

 Product lines can be used to make general conclusions about trade-offs between different features~\cite{sayyad13a,sayyad13b}. 
But it is harder to use
such product lines to make
predictions about, say, the expected software runtimes of planned products. Prior work on predicting runtimes from product lines had  to compile and execute
hundreds to thousands of specific products~\cite{guo2013variability}. 
Other studies, that tried to make do with   far fewer execution samples, could
be wildly inaccurate in their runtime predictions. For example, using  a pariwise sampling policy (where no pair of 
 variable settings should appear in more than one configuration) Seigmund et al. in 2012 built predictors for a product line description of the Apache web server~\cite{siegmund2012predicting}. Their resulting predictor had low mean errors ($\mu=15$\%)  but those predictions
 were very unstable. Specifically, their standard deviation
 was $\sigma=25\%$ which means that half they time, it is possible  they can be over 50\% incorrect\footnote{The
 25th to 75th percentile of  $N(\mu,\sigma)$ is   $\mu \pm 1.5\sigma$.
 For \mbox{$\mu,\sigma=15,25$}, this is $15 \pm 1.5*25$; i.e. 0 to 52.5\%.}.
 

This paper present ZIP, a {\em spectural clustering}  technique for  that generates very
accurate and very stable predictions of products generated from software product lines.
ZIP explores the spectrum (eigenvalues) of a distance matrix
between potential configurations to perform dimensionality reduction. That reduced
space is then queried to find a small number of representative examples which are used to
build a predictive model for runtimes (using a decision tree learner~\cite{breiman1984}).
For the product line descriptions tested here (Berkely database, the Apache web server, SQL, the LLVM compiler, the x264 video encoder),
those predictions have very low mean errors:
\[\mu \in \{2.7, 3.3, 5.6, 6.6, 9.3\}\%\]
as well as remarkably low  standard deviations
\[\sigma \in \{0.2, 0.3, 0.5,0.7,2,3,6,8\}\%\]
Note that those predictions were generated using   very few samples of runtime behavior
(we only had to collect runtime data on 16 to 64 executions).
This is most significant and most surprising  since the product lines explored here have up
to millions of possible variations. 

Note that this is a result with a large industrial utility.
In their recent paper ``Hey, You Have Given Me Too Many Knobs!: Understanding and Dealing with Over-Designed Configuration in System Software'', Xu et al.~\cite{xu2015hey} document the  difficulties developers face
with understanding  the configuration options of their systems.
Delivered systems have large numbers of configuration options, and the number of  configuration options tends to grow linearly
with time. Consequently, developers  struggle to understand both the current options as well as all the new options.
As a result, developers tend to ignore over $\frac{5}{6}$ths of the configuration options. 
This poor understanding of the space of configuration options has an economic cost:  for one system (Hadoop clusters),  Xu et al. report that ``configuration
issues are the dominant source of support cost''~\cite{xu2015hey}. 
Based on the Xu et al. study, it is clear that developers need better tools for understanding the 
consequences of all the configuration options (e.g. such as   the tools of this paper).



The rest of this paper is structured as follows. The next section introduces
product lines (expressed as feature models) and discusses why a spectral clusterer can
quickly sample those models. XXX 

\subsection{Preliminaries}
Before going on, we offer   some clarifying definitions.
By   ``predicting runtimes'' we more precisely 
mean predicting
runtimes of a test suite supplied with a system. Further, when we say ``predicting runtimes
of different configurations'', we mean predicting the runtimes of the different products
with respect to that test suite.  That is, we do not promise to predict runtimes with respect
to all possible inputs and operating system loads. Rather, we really mean predicting
the change in runtimes (on some fixed reference test suite) that result from using different
parts of a product line. These definitions tie our results to particular test suites.
If that test suite was to change, then a  new
runtime study would need  to be conducted. Ideally, such new predictors
can be quickly built using very few additional samples-- in which case, the methods
of this paper would be most relevant.

 
\begin{figure}[!t]
\includegraphics[width=1\linewidth]{Figures/BDBC.eps}
\caption{ Berkeley database feature model   (``C'' version). }\label{fig:bdbc}
\end{figure}
    


\begin{figure}[!t]
\scriptsize
\begin{tabular}{llllll}
  \hline
Project & Domain & Lang. & LOC & Features & Config\\\hline
BDBC: Berkeley DB   & Database & C & 219,811 & 18 & 2560\\
BDBJ: Berkeley DB   & Database & Java & 42,596 & 32  & 400\\
Apache & Web Server & C & 230,277 & 9 & 192\\
SQLite & Database & C & 312,625 & 39 & 3,932,160\\
LLVM & Compiler & C++ & 47,549 & 11 & 1024\\
x264 & Video Enc. & C& 45,743 & 16 & 1152\\\hline
\end{tabular}
 \label{fig:subjectsystems}
\caption{Feature models studied in this paper. For details on these systems,
see \fig{systems}.}\label{fig:cpm}
\end{figure}


\begin{figure}\small
\begin{tabular}{|p{.95\linewidth}|}\hline
\textbf{Berkeley DB CE} is an embedded database system written in C. It is one of the most deployed databases in the world due to its low binary footprint and its configuration abilities. We used the benchmark provided by the vendor to measure response time.

\textbf{Berkeley DB JE} is a complete re-development in Java with full SQL support. Similarly, we used a benchmark provided by the vendor measuring response time.

\textbf{Apache} is a prominent open-source Web server that comes with various configuration options. To measure performance, we used the tools autobench and httperf to generate load on the Web server. We increased the load until the server could not handle any further requests and marked the maximum load as the performance value.

\textbf{SQLite} is an embedded database system deployed over several millions of devices. It supports a vast number of configuration options in terms of compiler flags. As benchmark, we used the benchmark provided by the vendor and measured the response time.

\textbf{LLVM} is a compiler infrastructure written in C++. It provides configuration options to tailor the compilation process. As benchmark, we measured the time to compile LLVM's test suite.

\textbf{x264} is a video encoder in C that provides configuration options to adjust output quality of encoded video files. As a benchmark, we encoded the Sintel trailer (735\,\%MB) from avi to the xH.264 codec and measured encoding time.\\\hline
\end{tabular}
\caption{Systems studied in this paper.}\label{fig:systems}
\end{figure}



\section{Background} 

How can ZIP produce such stable predictions using so few samples? Our explanation is that,   due to the nature of the constraints within product lines and their resulting software, it is possible to quickly find a small
number of representative examples that are informative to the other possibilities.
To explain that idea, first we must discuss the nature of feature models.

\subsection{Product Lines as Feature Models}
For this paper, we represent product lines using Kang's feature model (FM) notation. Figure ~\ref{fig:bdbc}  shows the feature model of Berkeley DB(C Version) using the notations defined in ~\cite{kang1990feature}, ~\cite{guo2012consistency}. A feature model defines all valid feature combinations of a customizable system. As shown in the figure a feature model can be presented as a tree like structure that defines relationships among features.   The features on the software system is represented as a set of binary decision variables. A feature, when turned on, corresponds to 1, and 0 otherwise. All the features of a system is represented a vector F = $<f_1, f_2, ...,f_N>$, where N is the number of features of the software system. The dependent variable corresponding to a configuration(F) is a performance score, P(F), which the performance measurement.
    
    
Feature models can get far more elaborate than \fig{bdbc}. For example,
a feature model of the LINUX kernel~\cite{sayyad13b} contains thousands of features and hundreds of thousands of  {\em cross-tree constraints}; i.e. choices in one branch that rule out choices in other branches. 
Without those constraints, it takes linear time to generate possible products via a simple
top-down descent of the model. With those constraints, product generation becomes NP-hard and can
 defeat state-of-the-art theorem provers~\cite{pohl11} (particularly for larger feature models).
 
 
 
In practice, not all feature models are as complex as the LINUX kernel.
\fig{subjectsystems} details the feature models used in this study. While
they seem to be small (just a few dzen features), the last column in that table lists hundreds
to millions of configurations they can generate.


\subsection{Predicting Runtimes}
How can we predict the runtimes of products generated from such high-level descriptions
as \fig{bdbc}? One way would be compile all their variants and record the associated runtimes--
but this can be impractical.
For example, the software runtime data used in this paper required  26 days of CPU to collect (and much longer, if we also count the time required
 for compiling the code prior to execution). 
 Other researchers have commented that,  in 
 real world scenarios, the cost of acquiring optimal configuration is overly expensive and time consuming \cite{weiss2008maximizing}.
 
 When collecting runtimes on all configurations is impractical,  some {\em minimal sampling} policy
 is required to intelligently select just enough configurations should be executed.
 Zhang et al.~\cite{zhang2015performance} approximate the
 systems as a Fourier series, after which they can derive an expression showing how many configurations must be studies
 to build predictive models with error $\epsilon$. While a theoretically satisfying result, that approach still needs thousands to hundreds of thousands of executions of sample
 configurations.  Seigmund et al.~\cite{siegmund2012predicting} proposed heuristic
 sampling methods such as pair-wise sampling (where no pair of 
 variable settings should appear in more than one configuration). The  Seigmund et al. approach  requires far fewer runs than a Fourier-based analysis (dozens to hundreds of samples) but the results were highly unstable (large variances
 in the percent errors); e.g. recall the Apache example from the introduction.
 

\subsection{Spectral Clustering}\label{sect:spect}

The minimal sampling method explored in this paper is based on a spectral clusterering algorithm
that  explore the spectrum (eigenvalues) of a distance matrix between  configurations to  perform dimensionality  reduction.
Spectral methods have been used before for a variety of data mining applications.
Algorithms like PDDP~\cite{boley98} use spectral methods such as principle component analysis (PCA) to
recursively divide data into smaller regions.  Software analytics researchers like Theisen et al.~\cite{Theisen15} (amongst
others) use spectral methods (again, PCA) as a pre-processor prior to data mining in order to reduce noise in software-related data sets.
However, to the best of our knowledge, spectral methods have not been used before in software engineering as a basis 
of a minimal sampling policy.. 

Standard spectral method requires some $O(N^2)$ matrix multiplication to compute, say, the components
of PCA~\cite{ilin10}. Worse, in the case of hierarchical division methods like PDDP,
that polynomial time inference must be repeated at every level of the hierarchy.
Competitive results can be achieved
using an $O(2N)$ analysis we have developed previously~\cite{me12d}, that is  based on  a heuristic proposed by
Faloutsos and Lin~\cite{Faloutsos1995}
(which
Platt has
  shown computes a Nystr\"om
  approximation to the first component of
  PCA~\cite{platt05}).
  Our approach inputs
$N$
examples $N_1,N_2,..$. The FASTMAP heuristic
picks any
point $N_i$ at random then  finds
 the point  {\em West}~$\in N$ that is
furthest away\footnote{
 For this work, we use the standard Euclidean measure recommended for
instance-based reasoning by Aha et al.~\cite{aha91};
i.e. $\sqrt{\sum_i(x_i-y_i)^2}$ where $x_i,y_i$
are values normalized 0..1 for the range min..max.}
from $N_i$.
FASTMAP then
finds the point {\em East}~$\in N$
that is furthest from {\em West}.
We say that the line between {\em West} and {\em East} has  length  
$c=\mathit{dist}(\mathit{West},\mathit{East})$.

Our approach applies FASTMAP recursively to divide all the configurations as follows.
We iterate over $N_i \in N$
to find
$a=\mathit{dist}(N_i,\mathit{West})$,
$b=\mathit{dist}(N_i,\mathit{East})$,
$x=(a^2 + c^2 - b^2)/(2c)$.
This  $x$ value is the projection of $N_i$
on the line  running  {\em East} to {\em West}.  We divides
the examples on the median $x$ value,
then recurses on each half. Recursion on
$N$ initial
examples stops when a sub-region
contains less that  $M$ examples (e.g. 
$M=\sqrt{N}$).
Note that this process requires only $2N$ distance comparisons
per level of recursion, which is far less than the $O(N^2)$
required by PCA~\cite{Du2008}
or other clustering algorithms such as K-Means~\cite{hamerly2010making}.

\subsection{Sampling}
When recursive spectral clustering terminates, our  sampling policy (which we will call $S_1$) is then applied:
\begin{quote}
$S_1$: Compile and execute one  configurations,  picked at random, from each leaf cluster.
\end{quote}
We use this sampling policy since, later in this paper, we have also explored a range of alternate sampling policies
and found that $S_1$ performs better others"
\bi
\item $S_2$: East-West sampling: compile and execute the {\em East,West} poles of the leaf clusters;
\item $S_3$: Exemplar sampling: compile and execute all items in all leaves and return the one
with lowest runtime score.
\ei
Note that $S_2$ is {\em not} a {\em minimal} sampling policy (since it executes all configurations). 
We use it here as a baseline
against which we can compare the other, more minimal, sampling polices.

\subsection{Regression Tree Learning}
Using the data collected this way, we then use the CART regression tree learner \cite{breiman1984} to build a predictor for runtimes. Regression tree learners seek the attribute range split that most increases
our ability to make accurate predictions:
\bi
\item Let a split divide $n$ items  into   with $n_1$ and $n_2$ items
with standard deviations on the target variable of $\sigma_1$, $\sigma_2$.
\item The best split is the one that minimizes $\frac{n_1}{n}\sigma_1 + \frac{n_2}{n}\sigma_2$.
\ei
The best split CART divides the data on the best split seen in any attribute, the recurses into each split.
\subsection{Measuring Error}

The predictor built by CART  is then tested on data {\em not} used in the above spectral clustering processing (see below for details). For each each test item, we find out how long it {\em actually} takes to run and compare that
to the {\em prediction} from CART. The resulting prediction error is then computed using:
\begin{equation}\label{eq:err}
\mathit{error}=\frac{\mathit{abs}(\mathit{predicted} - \mathit{actual})}{\mathit{actual}}*100
\end{equation}
As reported in the introduction, the errors found in this way have very small mean errors   and even
smaller standard deviations (when the process is repeated multiple times across different random orderings of the data). To explain why these values are so low, it is important to understand what happens when data is processed by spectral
clustering.
When  data sets have many irrelevancies   or closely associated parameters then
only a few $e \ll d$ eigenvectors are required to characterize that data
since:
\bi
\item
Multiple inter-related variables $i,j,k \subseteq d$ can be represented
by a single  eigenvector.
\item
Noisy variables from $d$ are
ignored since they  do not contribute to the signal in the data.
\item
Variables  become (approximately) parallel lines
in $e$ space. For  redundancies \mbox{$i,j \in d$}, we
can ignore $j$
since effects that change over $j$ also
change in the same way over $i$.
\ei
Hence, our explanation for how 64 samples of the execution time of SQL (which has the
3,932,160 options listed in \fig{cpm}) is that most of those configuration options are either
closely associated or irrelevant to predicting the runtime of that system.
Hence, sampling a small number  configurations found via  spectral clusters returns
a  small number of most-informative examples that are surrogates
for many of the other legal configurations for the product line. 


 
\section{Research Questions}

In summary, ZIP is a combination of four methods:
(1) the FASTMAP methods of Faloutsos and Lin~\cite{Faloutsos1995};
(2)~a spectral clustering algorithm initially   inspired by    Boley's PDDP system~\cite{boley98}, which we modify
by replacing  PCA with FASTMAP (prior work have called this
method ``WHERE''~\cite{me12d});
(3)~the $S_1$ sample method that explores the leaf clusters found by this recursive division;
and (4)~the CART regression tree learner that converts the data from the samples collected by $S_1$
into a runtime prediction model~\cite{breiman1984}.
That is,
\begin{center}
WHERE = PDDP - PCA + FASTMAP\\\noindent
ZIP =   WHERE + $S_1$ + CART
\end{center}
To the best of our knowledge, this combination of methods has not been previously explored in the
software engineering literature. Also, ZIP is very different to other spectral
clustering method discussed in the machine learning literature:
\bi
\item
Those
machine learning applications explore image processing  while ZIP is a method for
definiting a minimal sampling policy to predict runtimes of SE systems;
\item
ZIP
has no need for the added complexity of a ``normalization matrix'' (see \tion{related} for details).
\ei

The rest of this paper is a certification study that checks the utility of ZIP. This study
is guided by the following concerns.

XXXX

Historical note: WHERE XXX

we ask for specific predictions about

siegmund2012predicting,guo2013variability}).

While
Feature models 

Search based software engineering for software product

such
as the runtimes of specific




All too often,
 software becomes  a victim of its own success.
 Once users find software useful, they request modifications to better suite
 their particular needs.
 This in turn leads to change requests to support multiple platforms or
 numerous specialized use cases. 
 To accommodate all these  requests,
 developers build complex {\em configuration files} that 
 control how software is customized and compiled. 
 
 These configuration
 files can grow so complex that it becomes difficult
 to understand how all the combinations of all the 
 configurations effect the system\cite{berger2013study}. 
Xu et.al \cite{xu2015hey} reports that users face tremendous difficulties to find the right configurations from the large configuration space. 

 In order for  users to find  optimal configurations for a particular job/functionality, they need an understanding of the dependencies between various configuration or perform an exhaustive search of all the configuration options. 
 A manual or exhaustive search of all those options
 is impractical; e.g.    an execution of all combinations of
 $n$ binary options results in  $2^n$   configurations, each of which might need compilation and tested. 
 
In practice,  not all $2^n$ configurations are valid; e.g. it makes no sense
 to configure a password encryption algorithm if a particular system is to be run inside a firewall (and so all passwords are disabled).  However, even after constraining option experimentation to just the valid combinations,
 the   runtimes required to (say) find the fastest configuration can be very long. 
 For example, the software runtime data used in this paper required  26 days of CPU to collect (and much longer, if we also count the time required
 for compiling the code prior to execution). 
 Other researchers have commented that,  in 
 real world scenarios, the cost of acquiring optimal configuration is overly expensive and time consuming \cite{weiss2008maximizing}.
 
 
 One way to reduce the time required to find an optimal configuration, is  to build an approximation of the software system that  can accurately 
 \bi
 \item {\em Predict} the  performance  of the software system (without having to actually execute the system). 
 \item {\em Optimize} that performance; i.e. extrapolating from that small sample, infer what other configurations   result in better  performance. 
\ei
Ideally, these {\em prediction} and {\em optimization} models can be build with least effort via  some {\em minimized sampling} across the space of possible configurations.
This paper proposes 
a novel {\em minimized sampling} policy, called   WAT\footnote{WAT is based on the WHERE clusterer~\cite{menzies2013local} and its name is short for ``Where u at''?} that recursively divides the space of configurations in two
until {\em leaves} are found of size $\sqrt{N}$ of the original data.
WAT then times how long it takes to executes software
built from two  samples selected at random from  each leaf. The resulting data is then
used to build a runtime predictor.  To assess the value of this approach, we ask three
research questions:
   
            {\bf RQ1:} {\em Can runtime prediction systems  be built using just a very small sample of configurations?} (in our case,  selected by WAT)
            In the following, we compare the number of samples used by WAT
              to the prior state of the art in this field: a pairwise sampling method proposed at ICSE'12 by Seigmund et al.~\cite{siegmund2012predicting}. 
              We show that WAT requires  25\% to 50\% fewer samples than that  prior
 work. The resulting sample sizes are very small indeed. For example, instead of exploring
 the millions of configurations available for SQL, WAT learns good runtime predictors using
 just 64 executions.
            
            {\bf RQ2:} {\em Does less data used in building the models lead to less
            effective predictions?}  This is a concern since the {\em less} you sample data,
            the {\em less} you constrain a model built from those samples. Therefore,
            our pre-experimental concern with WAT was that it would sample too
            little, thus resulting in an under-constrained prediction model that generated
            wildly varying predictions.  This paper finds that  WAT's predictions are effective in both the absolute and relative sense. Compared to those of 
            Seigmund et al., WAT's  errors are small (less than 10\% relative
            to the actual) while in relative terms, our preidcions are 
 four to sixteen times more accurate that those of Seigmund et al. (measured in terms of mean or standard deviations of the errors)
          
          {\bf RQ3:} {\em Can optimizers use prediction models of runtimes
            to infer what other configurations result in better  performance?} 
           Another issue with reasoning from very small samples is that it is
           possible to overlook some important
           part of the phenomena being studied; e.g. the configuration that
           leads to fastest runtimes. Hence, it is important to check if those
           faster configurations can be inferred from (say) the 64 sampled configurations
           we explored for SQL. To this end, we (a)~built runtime predictors using, say, 64 samples then (b)~used those predictors to guide optimizers looking for faster
           configurations (whenever
           the optimizer had to assess a new configuration, it   asked the predictive
           model). When compared to  a ``ground truth'' library containing up to thousands of executed configurations, the configurations found in this way were always amongst the fastest items seen within the ``ground truth''.
           
           
           When business users are offered a prediction that they do not like, their next question is usually ``how can we change that''?
            is 
            This question arose from  optimization can be connected to performance-- a good performance predictor can be used 
to assess novel configurations proposed by an optimizer.
Further,  WAT supports an optimizer that can infer other configurations (not seen inthe sampled configurations) that result in faster runtimes.
 
 The rest of this paper is structures as follows. 
        \ei
        
The rest of the paper is organized as follows. Section 2 introduces the background of the challenges we are trying to tackle. Section 3 formalizes the problems and describes the datasets used for the experiments. Section 4 presents the sampling techniques and section 4 describes a fast optimizing algorithm, GALE. The paper concludes after discussing the issues relating to reliability and validity in section 6. 
 
 
 
 \subsection{Contributions of this Paper}
 Prior work on building  approximations for the runtime behavior of software systems has explored {\em prediction}~\cite{guo2013variability,siegmund2012predicting,westermann2012automated,sarkar2015cos,zhang2015performance}
 but not {\em optimization}. In a  brute force approach, analysts  explore a large number
 of configurations, collecting runtimes for each one.
 Using that brute force approach,  it is possible to learn a decision tree that very accurately {\em predicts} the runtime of new configurations~\cite{guo2013variability}. 
 When  brute force is impractical,  
 various {\em minimized sampling} policies can be applied.  Zhang et al.~\cite{zhang2015performance} approximate the
 systems as a Fourier series, after which they can derive an expression showing how many configurations must be studies
 to build predictive models with error $\epsilon$. While a theoretically satisfying result, that approach still needs thousands to hundreds of thousands of sample runtimes. In 2012,  Seigmund et al.~\cite{siegmund2012predicting} proposed heuristic
 sampling methods such as pair-sampling (where no pair of 
 variable settings should appear in more than one configuration). The  Seigmund et al. approach  requires far fewer runs than a Fourier-based analysis (dozens to hundreds of samples) but the results were highly unstable (large variances
 in the percent errors).

 
 This paper proposes {\em WAT}:  a novel minimized sampling method.
 WAT uses a top-down bi-clustering algorithm to splits  configurations in half along a line
drawn between two  distant configurations (and by ``distant'', we mean least number of
identical configuration options). 
WAT then recurses on each half, stopping when a split is {\em final};
i.e. it contains fewer than some  $\mathit{MIN}$ minimum number of configurations (e.g $\mathit{MIN}=\sqrt{N}$ of
the number of possible configurations).
On termination, WAT finally returns two randomly selected configurations per final split,
from which:
\bi
\item We can build a {\em predictive} model;
\item Drive an {\em optimizer} that uses the predictive model to infer what other configurations result in better performance.
\item Which is all done using {\em minimized sampling} of the possible evaluations;
e.g. if $\mathit{MIN}=\sqrt(N)$ then WAT needs only $2\sqrt{N}$ samples).
\ei
This paper shows that {\em WAT} is better than the state-of-the-art system
previously propse by Seigmund et al. The predictions learned from WAT's
samples are more   accurate (i.e. low mean error) and  more stable (i.e. low standard deviation):
\begin{center}
\begin{tabular}{rr@{~}lr@{~}l}
         & \multicolumn{2}{c}{usual case} & \multicolumn{2}{c}{worst case} \\
  Source & \multicolumn{2}{c}{(50th percentile)}    & \multicolumn{2}{c}{(100th percentile)} \\\hline
 Seigmund et al. &  $7.4$& $\pm \;10$\%  &  $44.0$ & $\pm 42.3$\%\\
WAT &  $6.1$& $\pm \;\;0.6$\% &  $9.9$&  $\pm$~~ $6.8$\%\\\hline
ratios   & 1.2 & ~~~~16.7          & 4.4 & ~~~~~~6.2
  \end{tabular}
  \end{center}
In the above $X \pm Y$ shows mean and  standard deviation values seen across various trials. Note that
 WAT's mean errors are   1.2 to 4.4 times smaller and
 the standard deviations are 16.7 to 6.2   times smaller than the prior state of the art in this 
 research area.
 
Apart from generating more accurate and stable predictions,  
another reason to recommend WAT is that, compared to all the methods described above~\cite{%
siegmund2012predicting,%
zhang2015performance,%
guo2013variability,%
westermann2012automated,%
sarkar2015cos}
WAT requires
the least number  samples for prediction and optimization.
For example WAT requires  around   $\frac{1}{4}$th to $\frac{1}{2}$th
of the samples needed by  Seigmund et al.

\begin{comment}


Given two distant configurations $C_0,C_1$ in a final split,
we compile and execute $C_0,C_1$ then mutate  all
all configurations in that split towards the fastest one. This method can hence infer configurations
not seen in the original, sample that that run even faster than any example in that sample.
We show below that this optimizer can find some of the fastest possible configurations for the systems
studied here.
 
 
Our goal is to build performance models that can be trained using the few training samples, which then can be used, along with a fast optimizer, to find optimal configuration of the system (for eg. minimize run time of a job).  To further emphasize that it is expensive to collect complete configuration space (exhaustive search), let us look at an example. Assume that there is a software system with 9 features (configuration space = $2^9$) and a job requires at an average 20 minutes to execute. The total time required to find an optimal configuration, using exhaustive search, is 4300 minutes ($2^9 \times 20$). This problem can be solved in a short amount of time, if the user can build a performance model with fewer samples ($\ll 2^n$). From a learner perspective, our goal can be described as a sampling technique, which can determine "good sample" for a given system. A "good sample" can be described as a sample if it is small enough to decrease the measurement effort and large enough to increase the prediction accuracy of the model. We propose three new sampling techniques, which uses a clustering technique and use point/s from clusters to build the sample set. We conduct experiments on six real-world configurable system to compare the sampling strategies in terms of total time saved by the sampling techniques wrt. to the baseline strategy ($2/3$ of the sample size).
\end{comment}


 \section{Misc-- not sure we need to use}
 
 The big question for your approach is, however, how should we rate the quality (e.g., accuracy, feasibility, usefulness) of a solution (i.e., the estimated optimal configuration) 
\bi
\item Valid;
\item Compared with ground truth.
\ei
The first point can be addressed using {\em feature models} which are a lightweight notation
for describing valid combinations of configurations within a  system~\cite{kang1990feature}. In the rig
described below, our optimizer is programmed to reject all configurations that violate
feature models describing valid Makefile settings.

As to the second point, to generate that ground truth we can generate and compile and execute
a  very large set of configurations. Within that set, there exists some fastest runtime $T_0$--
and we would assert our optimizer is useful if it can recommend a system that executes
with time $T_1 \approx T_0$.

As an aside, we note that when discussing this rig with colleagues, they ask if we have the ground truth, why do any optimizations? The answer is that the validation rig used
in this paper is {\em different} to how this this method would be applied {\em after}
it has been validated. It is true that to {\em validate} this approach, we must compile
and execute hundreds to thousands (or more) variations of the Makefiles. Using that information,
we can test what would happen if:
\bi
\item We sample just a few variations (say, $N=64$ or less);
\item Then build an optimizer to study those variations is order to propose a best, most fastest configuration;
\item Then compile that proposed configuration to find $T_1$ (the runtime of the compiled code generated from that proposed configuration).
\ei
To assess $T_1$, we need to compare it to the $T_0$ (the  the fastest runtime of any configuration). Hence, to validate the rig, we need many configurations. However, given the results of this paper, we assert that it
in practice, it
would be acceptable to quickly sample (say) $N=64$ configurations (selected in an intelligent manner,  using the methods described below).

~\hrule~
\section{Background}

\textbf{Feature Maps}

    \textbf{Sampling: } Figure ~\ref{fig:GeneralProcess} illustrates the general process of performance prediction using sampling. It starts with an initial sample of measured configurations, which are used to build the prediction model. A good initial sample significantly reduces the effort involved with training sample collection. The boxes in yellow describes the current state of the art sampling process, where as the boxes in grey are the new approach proposed in the paper. 
    
    The state of the art approaches involves progressive and projective sampling techniques \cite{sarkar2015cost} and Fourier Transforms \cite{zhang2015performance}. However these approaches assumes the user to know a lot of about the system, which is not a fair assumption. The problem with iterative techniques lies in the step size, which is an engineering decision(aka black art). In our approach, we use clustering, which uses the first principle component, to determine clusters rather than relying to random sampling methods. Once the clusters are obtained, \'interesting\' representative/s from each cluster  is/are collected. Then these points are used to build prediction models using a statistical learning technique called Classification and Regression Tree (CART), which has been demonstrated to be fast and accurate for performance prediction of configurable system \cite{guo2013variability}.
    
In our work, we use the time saved by the proposed sampling when compared to the exhaustive search is presented along with the prediction accuracy (similar to ~\cite{guo2013variability}, \cite{siegmund2012predicting}, \cite{westermann2012automated}). 
    \begin{figure}[!t]
\includegraphics[width=\linewidth]{Figures/GeneralProcess.png}
\caption{ General Process of Performance Modelling. }\label{fig:GeneralProcess}
\end{figure}


    \textbf{Search Algorithms: }Linear time algorithms works well, when the problem scenario is same as the scenarios where the algorithm was developed. But in the real work scenario, the scenarios presented are slightly different from what the algorithms were developed in. For example, one particular technique is available to calculate the minimum-cost allocation of resources for a problem where both the cost function and constraints are linear. The method is fast and reliable. And it is almost applied inappropriately  in real-world settings where the cost function and constraints are almost always nonlinear. In essence, anyone using this method runs the risk of generating the right answer to a problem that does not exist \cite{michalewicz2013solve}. 
    Another challenge faced by the deterministic algorithms lie in the fact the each new solution
relies on a single solution as the basis for future exploration with each iteration. They either process complete solutions in their entirety, or they construct the final solution from smaller building blocks. Greedy algorithms,
Dynamic programming, Branch and bound methods etc. 

Evolutionary algorithms abandons this idea totally and considers multiple solutions at the same time. Evolutionary algorithm starts with an \textit{initial population} (randomly generated, based on some constraints). Since the solutions are drawn from a uniform distribution, typically, this diversity is desirable, although in some cases we might want to initialize all of the available parents to some best-known solution and proceed from there. The chosen \textit{evaluation function} must be capable of differentiating
between two individuals, i.e., it has to be able to rank one solution ahead
of another. Those solutions that are better, as determined by the evaluation
function, are favored to become parents for the next generation of offspring. 
\textit{Reproduction} refers to  new solutions being generated probabilistically in the neighborhood of old solutions. This process continues till the a \"good enough\" solution is achieved or a hard limit on iterations are reached. This criteria is called the \textit{stopping criteria}.
    
    There has been extensive research in evolutionary algorithms in the recent decades. For example, here is a list of search algorithms used widely in research: \textit{simulated annealing}\cite{bell2013limited, menzies2007business}; various genetic algorithms\cite{goldberg1979complexity} agumented by techniques such as \textit{differential evolution} \cite{storn1997differential}, \textit{tabu search and scatter search}\cite{nebro2008abyss, molina2007sspmo, glover1986general, beausoleil2006moss}; \textit{particle swarm optimization}\cite{pan2008particle}; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods\cite{krall2014gale, zuluaga2013active}. GALE, short for Geometric Active Learning Evolution,
combines spectral learning and response surface methods
to reduce the number of evaluations needed to assess a set
of candidate solutions. The algorithm is an active learner;
i.e. instead of evaluating all instances, it isolates and explores
only the most informative ones. We explain the working of GALE in section 5.

\input{tex/vs2102}


\section{Problem Formulation}


    Configurations of a software system may not be independent, such that we cannot measure arbitrary configurations. Researchers have explored the presence of complex domain dependencies. With constraint between features, in theory there can be multiple minimal configuration (for example, in the presence of mutually exclusive features). These dependencies are captured well by \textit{Feature Models(FM)}. FM defines all the N features and all the valid configurations C of a software system. Figure ~\ref{fig:bdbc}  shows the feature model of Berkeley DB(C Version) using the notations defined in ~\cite{kang1990feature}, ~\cite{guo2012consistency}. A feature model defines all valid feature combinations of a customizable system. As shown in the figure a feature model can be presented as a tree like structure that defines relationships among features. The model can be used to check the validity of the solution produced during the mutation phase of the learners.
    
\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/BDBC.eps}
\caption{ Berkeley database feature model   (``C'' version). }\label{fig:bdbc}
\end{figure}
    
    The features on the software system is represented as a set of binary decision variables. A feature, when turned on, corresponds to 1, and 0 otherwise. All the features of a system is represented a vector F = $<f_1, f_2, ...,f_N>$, where N is the number of features of the software system. The dependent variable corresponding to a configuration(F) is a performance score, P(F), which the performance measurement.

    Performance of system can be modelled using the configuration as the independent variable and the performance metric as a dependent variable. 
    Modelling a software system is to find a hypothesis function(h), such that h(F) predicts P(F) accurately. This can be represented as:\\
\begin{center}
    $ h: C \mapsto \rm I\!R$ s.t. $L(h(F), P(F))$ is minimized\\
\end{center}

where, L is a loss function for penalizing errors in prediction. By formulating the performance prediction using the method described, we can transform the problem into a learning problem. 
    It should be noted that though the number of possible configurations (C) is equal to $2^N$(binary decision variables), in reality valid configurations(\^{C}) is smaller than C (|C| << |\^C|). 


To answer our research questions, we use data set from our previous work on detecting performance interactions~\cite{SKR+12}. It contains performance measurements of real-world configurable software systems.
These systems were selected to cover a broad range of domains (databases, Web servers, video encoder, compiler), programming languages (C, C++, Java), and different sizes regarding number of features and configurations. Table~\ref{tab:subjectsystems} provides an overview of these systems.
 
 
The data in these
data sets have a continuous class (run time of the compiled system)
so the performance of a quality predictor can be measured in terms
of difference between the predicted runtime $p$ of test case items
and their actual runtimes $a$ using 

\begin{equation}\label{eq:1}
MRE = \frac{abs(s-a)}{a} \times 100
\end{equation}


For all systems, except for SQLite, we obtained the whole population data (i.e., all valid configurations). For SQLite, we measured all configurations corresponding to one-way and two-way interactions and additionally sampled 100 random configurations.

To evaluate our approach, we  implemented our techniques in Python and run it against data constructed from real-world software systems across different domains. 

\section{Sampling Technique: East-West Where}
% \begin{figure}[!t]
% \includegraphics[width=0.9\linewidth]{Figures/SamplingProcess.png}
% \caption{Description of Sampling Process. Part 1 describes the training process where as, part 2 describes the testing process.}\label{fig:Sampling Process}
% \end{figure}
\subsection{Approach}
This section describes how a large space of candidate solutions (configurations) is clustered into many smaller clusters.
\subsubsection{WHERE Learning}\label{sec:spectral}


WHERE is a {\em spectral learner}~\cite{kamvar2003spectral}; i.e. given solutions with $d$ possible decisions(features), it re-expresses those $d$ decision variables in terms of the $e$ eigenvectors of that data.
This speeds up the reasoning since we then only need to explore the $e\ll d$   eigenvectors.

A widely-used spectral learner is a principal components analysis (PCA). For example, PDDP ({\em Principal Direction Divisive Partitioning})~\cite{boley1998principal} recursively partitions data according to the median point of data projected onto the first PCA component of the current partition.

WHERE~\cite{me12d} is a linear time variant of PDDP  
that uses FastMap~\cite{Faloutsos1995} to quickly find the first component.
Platt~\cite{platt05} shows that FastMap is a  Nystr\"om algorithm that finds approximations to eigenvectors.
As shown in \fig{fastmapCode} on lines 3,4,5, FastMap  projects all data onto a line connecting two distant points(poles)\footnote{
To define distance, WHERE uses the standard Euclidean distance method proposed by Aha et al.~\cite{aha91}; that is: $dist(x,y)= \sqrt{\sum_{i\in d} (x_i - y_i)^2}/\sqrt{ \left\vert{d}\right\vert }$ where distance is computed on the independent decisions $d$ of each candidate solution; all $d_i$ values are normalized min..max, 0..1; and the calculated distance normalized by dividing by the maximum distance across the $d$ decisions.}. 
FastMap finds these two distant points in near-linear time. 
The search for the poles needs only $O(N)$ distance comparisons (lines 19 to 24).
The slowest part of this search is the sort used to find the median $x$ value (line 10) but even that can be reduced to  asymptotically optimal linear-time via the standard median-selection algorithm~\cite{hoare61}.

\begin{figure}[!t] 
\begin{minipage}{3.2in}
\begin{lstlisting}[mathescape,frame=l,numbers=left]
def fastmap(data): 
  "Project data on a line to 2 distant points"
  z          = random.choose(data)
  east       = furthest(z, data)
  west       = furthest(east, data)
  data.poles = (west,east)
  c          = dist(west,east)     
  for one in data.members: 
    one.pos = project(west,east,c,one)
  data = sorted(data) # sorted by 'pos'
  return split(data)

def project(west, east, c, x): 
  "Project x onto line east to west"
  a = dist(x,west)
  b = dist(x,east)
  return (a*a + c*c - b*b)/(2*c) # cosine rule

def furthest(x,data): # what is furthest from x?
  out, max = x,0
  for y in data:
    d = dist(x,y)
    if d > max: out, max = y, d
  return out

def split(data): # Split at median
   mid = len(data)/2; 
  return data[mid:], data[:mid]
\end{lstlisting}
\caption{Splitting data with FastMap}
\label{fig:fastmapCode}  
\end{minipage}
\end{figure}

FastMap returns the data split into two equal halves.
WHERE recurses on the two halves, terminating when some split has less than $\sqrt{N}$ items. To summarize, the WHERE module of the technique takes data points as inputs and returns $2^{(1+log(\sqrt{N})))}$ number of clusters.

Next two subsections describes various technique of selecting points in such a way that the point select is an representative of the cluster

\subsubsection{WHERE Exemplar}\label{where_exemplar}
This technique borrows from WHERE spectral learner discussed above and uses the clusters to perform clustering. WHERE splits the data into smaller clusters, each of which is characterized by two distant points (poles). 
 The technique assumes the best point (exemplar) as the representative of each cluster. This technique uses one solution from each cluster(best) and hence $2^{(1+log(\sqrt{N})))}$ number of training examples, where M is the size of the configuration space.

\subsubsection{WHERE East West}\label{where_east_west}
This technique as the above technique is also uses the WHERE spectral learner as a precursor to sampling. 
It is assumed  that the technique only needs to evaluate
the most informative subset consisting of the poles used to
recursively divide the data. The recursive binary division of the solutions(M), and that this technique uses two solutions in each cluster, which means the technique uses only $2*2^{(1+log(\sqrt{N})))}$.

\subsubsection{WHERE Random}\label{where_random}
This technique uses WHERE spectral learner to cluster the data points and randomly chooses a point from the clusters. It is assumed that a random point in the cluster can be used as a representative to all the points in cluster. This technique only requires to one evaluation per cluster and hence $2^{(1+log(\sqrt{N})))}$ number of training examples.\\

We use the points sampled, using the technique described above, to build the predictive model using the popular machine learning technique called Classification and Regression Tree(CART), which has been extensively used in the literature and has been observed to be fast and accurate~\cite{guo2013variability}.

\subsection{Results}

Using the experiments, we try to make a case for "less quantity, more data". We consider our technique as variant of a compression algorithm. The clustering algorithm (WHERE), find clusters of data points (clustered on the first principle component) with similar properties. Sampling from every cluster means we are including all the variations presented in the dataset. 
By conducting the experiment on six real world datasets we try to answer the following research questions.

\textit{A. Can a reasonable prediction accuracy be achieved based on 
subset of a training set?}\\
\\
Figure ~\ref{fig:sampling_accuracy} show us how accuracy measured in terms of MRE, measure when the size of the dataset used is varied. X-axis represents the size of the dta used as a training set. For eg. Apache has 192 instances (all possible combinations), when 20\% of the data is used. It means 38 instances are chosen at random(without replacement) and this subset of the dataset is used as an input to the sampling techniques. In the figure we see that WHERE east-west and WHERE random are achieving the same accuracy, while WHERE exemplar is consistently performing worse than the other methods. We also observe is sampling on 40\% of the data is enough to model the configuration space. 

Since our objective is to reduce th enumber of examples required to train a model, it is important to compare the number of examples required to train a model(CART). In figure ~\ref{fig:Evaluations}, we see that WHERE random and WHERE east-west use far less number of evaluations as compared to the WHERE exemplar. From this point forward we don't consider WHERE exemplar while comparing different approaches, since it requires more number of evaluations and don't do as well as other techniques. 

Figure ~\ref{fig:Runtimes} shows that WHERE random and WHERE east-west, both techniques require less than 10\% and 20\% (when compared to exhaustive search) respectively. The numbers on the top of the cluster bars are the total time required to perform an exhaustive search. For eg. to execute all the possible combinations of configuration of Apache requires 78 hours, where  WHERE random can build a model in less that 5 hours. 

\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/SamplingAccuracy.eps}
\caption{Behavior of accuracy of WHERE-exemplar, baseline, WHERE-east-west wrt. to different training set sizes }\label{fig:sampling_accuracy}
\end{figure}



    \begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/evaluation_graph.eps}
\caption{ Comparing evaluations of different approaches.  }\label{fig:Evaluations}
\end{figure}

    \begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/sum_of_run_times_graph.eps}
\caption{ Comparing the runtimes of two promising approaches }\label{fig:Runtimes}
\end{figure}


\textit{B. Does less data used in building the models lead to large
variance in the predicted values?}\\

We have shown with sufficient evidence that models can be trained using less data, while achieving the accuracy
attained by using larger training data. 
The another aspect, which needs to be considered at this point is the variability that is introduced  when a sampled dataset used to train CART. 
Figure ~\ref{fig:Variance} shows the variance is under 10\%, when the sampling techniques use atleast 40\% of the data. 
\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/Variance.eps}
\caption{Variances }\label{fig:Variance}
\end{figure}

In Figure \ref{fig:time_saved}, we show the amount of time saved by various sampling methods. The x-axis of the graph represent the percentage of data used to build the CART model. The y-axis shows the time saved by the various sampling methods. There are take away from the figure are:
\bi
    \item{The sampled population remains constant even if the amount of training set increases. This shows that the method is scalable and only depends on the number of features in the dataset rather than the number of rows.}
    \item{From the previous sections, we know that \textit{WHERE east west} achieves 'almost' same accuracy,
    which mean the technique saves time while preserving accuracy and reducing variability.}
\ei


These results, along with the fact that the size of the sampled population is smaller compared to the configuration space, shows us that the \textit{WHERE east west} can indeed save time and effort, when the cost of collecting training samples is expensive.

\section{Fast Searcher: GALE}
Once we have the software performance model build using any sampling technique, the obvious next step is to find the optimal configuration such that the performance score of that particular configuration is the minimal. At this point, we can use any searcher to find the optimal configuration of a system(modelled by CART). In this section, we introduce GALE, which has proven to find near optimal solution with minimum model evaluations as possible. 
\subsection{Approach}
\begin{figure}[!b]
\small
\begin{tabular}{|p{.95\linewidth}|}\hline
GALE initially builds a population of points by selecting decisions at random. It then {\em clusters} those decisions into neighborhoods as follows:
\begin{enumerate}
\item Find two distant points in that population; call them the {\em east} and {\em west} poles. 
\item Draw an axis of length $c$ between the poles. 
\item Let each point be at distance $a,b$ to the {\em east,west} poles.  Using the cosine rule, project each point onto the  axis  at $x=(a^2 + c^2 - b^2)/(2c)$.  
\item Using the median $x$ value, divide the population.
\item For each half that is larger than $\sqrt{N}$ of the original population, go to step 1.
\end{enumerate}

Note that the above requires a distance measure between sets of decisions: GALE uses the standard case-based reasoning measure defined by Aha et al.~\cite{aha91}. Note also that GALE implements step1 via  the FASTMAP~\cite{Faloutsos1995} linear-time
heuristic:
\begin{itemize}
\item Pick any point at random; 
\item Let {\em east} be the point furthest from that point; 
\item Let {\em west} be the point furthest from {\em east}.
\end{itemize}

These final sub-divisions found by this process are the {\em neighborhoods} that GALE will {\em perturb} as follows:
\begin{itemize}
\item Find the objective scores of the {\em east,west} poles in each neighborhood.
\item Using the continuous domination predicate of \fig{moea}, find  the {\em better} pole. 
\item Perturb all points in that neighborhood by pushing them towards the better pole, by a distance  $c/2$ (recall that  $c$ is the distance between the poles).
\item Let generation $i+1$ be the combination of all pushed points from all neighborhoods.
\end{itemize}

From a formal perspective, GALE is an active learner~\cite{Dasgupta2005} that builds a piecewise linear approximation to the Pareto frontier~\cite{Zuluaga:13}.  
For each piece, it then pushes the neighborhood up the local gradient.  This  approximation is built in the reduced dimensional space found the FASTMAP  Nystr\"om approximation to the first component of PCA~\cite{platt05}.
\\\hline
\end{tabular}
\caption{Inside GALE}\label{fig:gale}
\end{figure}
GALE combines (a) the neighborhood perturbation  with (b)~the MOEA algorithm of \fig{moea}.  The algorithm reflects over a {\em population} of points, each of which contains {\em decisions} (configurations of a software system).  It then searches for the input decisions that lead to best outcomes.  For example:
\begin{itemize}
\item we adjust the inputs to CART (modelling the performance score of the software system)
\item GALE can report the best configuration for the system such that the performance score is (near) lowest.
\end{itemize}

\fig{gale} lists the procedure by which GALE clusters the data into neighborhoods, then perturbs each neighborhood.  In terms of monitoring for brittleness, the key point of GALE is that this process continues until the perturbations stop having any new effect (i.e. they stop generating lower performance scores). That is, all GALE solutions are guaranteed not to be brittle.

\begin{figure}[!t]
\small
\begin{tabular}{|p{.95\linewidth}|}\hline
An evolutionary multi-objective optimization algorithm requires at least two  operators:
{\em cull} and {\em perturb}:
\begin{enumerate}
\item Generate an initial population by randomly selecting decisions;
\item {\em Cull} the individuals with the lower objective scores.
\item Generate a new population $P_n$ by {\em perturbing} the decisions of the surviving individuals (e.g. via random mutation or grafting together parts of the decisions of different individuals)
\item Halt if $P_n$ no better than prior generations $P_{m<n}$.
\item Else, go to step 2.
\end{enumerate}
One way to implement the culling (step 2) is via {\em domination}; i.e. remove one example if it can be shown that it is worse that (a.k.a. ``is dominated by'') some other examples. 

Two forms of domination are {\em binary} and {\em continuous} domination.  In {\em binary domination}, one individual $x$ dominates $y$ if all $x$'s objectives are never worse than  the objectives in  $y$ but at least one objective in solution $x$ is better than its counterpart in $y$; i.e.
\[ \begin{array}{c}
\left\{ 
     \forall o_j  \in \textit{objectives}\;\mid\; \neg ( o_{j,x} \prec o_{j,y}) \right\} 
\\
 \left\{
\exists o_j \in \textit{objectives} \;\mid\; o_{j,x} \succ y_{j,y}\right\}
\end{array}\]
 where ($\prec,\succ$) tests if an objective score in one individual is (worse,better) than in the other individual.

An alternate culling method is the {\em continuous domination} predicate~\cite{Zitzler04indicator-basedselection} that favors $y$ over $x$ if $x$ ``losses'' least: 
\begin{equation}\label{eq:cdom}
\begin{array}{rcl}
\textit{worse}(x,y)& =& \textit{loss}(x,y) > \textit{loss}(y,x)\\
\textit{loss}(x,y)& = &\sum_j^n -e^{\Delta(j,x,y,n)}/n\\
\Delta(j,x,y,n) & = & w_j(o_{j,x}  - o_{j,y})/n
\end{array}
\end{equation}
where  ``$n$'' is the number of objectives and $w_j\in \{-1,1\}$ depending on whether we seek to maximize goal $x_J$.  
\\\hline
\end{tabular}
\caption{Inside an MOEA.}\label{fig:moea}
\end{figure}

In terms of reducing runtime, the key feature of GALE is that unlike traditional MOEA methods such as NSGA-II~\cite{deb00afast}, GALE  does not automatically generate objective scores for all $N$ decisions.  Instead, as it recursively clusters the data in two (using steps 1,2,3,4,5 in \fig{gale}), GALE only computes the objective scores for the two most distant points in each division.  This means that this binary division of the data terminates after a comparison of just $log_2(N)$ evaluated individuals. This is much less than the $2N$ comparisons required by  traditional methods like NSGA-II.

The perturbing module of GALE was modified for this problem. Since this problem has a constrained solution space, the perturbing module was made sensitive by including the feature model so, that GALE can recognize invalid solutions . If the solution generated after perturbing the solution, is not a valid solution, the solution is not considered for future perturbation.

\subsection{Results}

\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/optimizer_result.eps}
\caption{Solutions found by GALE}\label{fig:performance_graph}
\end{figure}



We try to answer the following research question in this section\\

\textit{C. Can evolutionary algorithms be used to find the optimal
configurations using the model generated?}
\\
Figure ~\ref{fig:performance_graph} presents the results after running GALE with the performance model. The figure shows the distribution of the performance score (sorted in the ascending order). In the figure we would like to show that GALE has been able to find the configuration/s with (almost)minimal performance scores. The graph shows the distribution of the scores.  At this point, we would like to point out the GALE doesn't always find the optimal solutions but find solutions close to the optimal. For example, GALE is about to find the third best configuration for SQL lite. The blue, red and yellow dot represents the 25, 50 and 75 percentile solutions found by GALE. It is to be noted that the solutions found by GALE are relatively stable(variance of the solutions found are low) and is very close to the actual medium. For SQL, the solutions found couldn't be compared with the ground truth since the configurations were not available(given that an exhaustive search was not possible since number of configurations is 39). We show here that using the software model build using \textit{WHERE random}, we have been able near optimal solution. Similar results can be found by models build using the \textit{WHERE east-west}. We believe that this further validates the usefulness of our approach.  


 \section{Related Work}
 \subsection{``Spectral Clustering'' in the  Machine Learning Literature}\label{tion:related}
 
In 2000, Shi and Maik~\cite{shi00} claimed the term ``spectral clustering'' as a reference to their normalized cuts
image
segmentation algorithm that  partitions data through a spectral (eigenvalue) analysis of the  
Laplacian representation of the similarity graph between instances in the data.

In 2003, Kamvar et al.~\cite{kamvar2003spectral},  generalized that definition saying that ``spectral learners''
were any data mining algorithm that first replaced the raw
dimensions with those inferred from the spectrum (eigenvalues) of the affinity (a.ka. distance)
matrix of the data, optionally adjusted via some normalization technique).

Our
top-down divisive bi-clustering splits the data on a   approximation to an eigenvector, found at each recursive level
of the data (as described in \tion{spect}). 
Hence, this  method is a ``spectral clusterer'' in the general Kamvar-sense. 
Note that
for software engineering data, we have
not found that Kamvar's normalization matrixes are needed (but perhaps if we were text mining
on very large dimensional data, we would add in that normalization). 
 


\section{Reliability and Validity}\label{sect:construct}

{\em Reliability} refers to the consistency of the results obtained
from the research.  For example,   how well independent researchers
could reproduce the study? To increase external
reliability, this paper has taken care to either  clearly define our
algorithms or use implementations from the public domain
(SciKitLearn). Also, all the data used in this work is available
on-line in the PROMISE code repository and all our algorithms
are on-line at github.com/ai-se/where.

{\em Validity} refers to the extent to which a piece of research actually
investigates what the researcher purports to investigate.
{\em Internal validity} checks if the differences found in
the treatments can be ascribed to the treatments under study. 

One internal validity issue with our experiments is the choice
of {\em training and testing} data sets discussed in 
\tion{design}. Recall that while all our learners used the same
{\em testing} data set, our untuned learners were only given
access to {\em training}.

Another internal validity issues is {\em instrumentation}. The very low $\mu,\sigma$ error values
reported in this study are so small that it is reasonable to ask they are due to some instrumentation
quirk, rather than due to using a clever sample strategy:
\bi
\item
We note that our low $\mu$ values are consistent with prior work.  Sarkar et al.~\cite{sarkar2015cost} report their CART predictions
(learned using all configurations executed and the runtimes collected) had  errors of around 4\% (calculated using \eq{err}). Recall from our introduction that that  $\mu$ error values  seen with the methods of this paper
can be slightly more than 4\%-- which is to be expected since our predictive models were built using less
data. 
\item
As to our low $\sigma$ values, we note that when the  error values are so close to 0\%, the standard
deviation of the error is ``squeezed'' between zero and those errors. Hence, we would expect that
experimental rigs
that generate error values on the order of 5\% \eq{err} should have $\sigma$ values of $0\le \sigma \le 5$ (e.g. like those seen in our introduction).
\ei


XXX need some earlier notes on SQLlite \newline Another issues about {\em Internal validity}
Regarding SQLite, we cannot measure all possible configurations in reasonable time. Hence, we sampled only 100 configurations to compare prediction and actual performance values. We are aware that this evaluation leaves room for outliers.
Also, we are aware that measurement bias can cause false interpretations [20]. Since we aim at predicting performance for a special workload, we do not have to vary benchmarks.



{\em External validity}  We aimed at increasing the external validity by choosing programs from different domains with different configuration mechanisms and implemented with different programming languages. Furthermore, the programs used are deployed and used in real world. Nevertheless, assuming the evaluations to be automatically transferable  to all configurable programs is not fair. To further strengthen the external validity we run the model(generated by \textit{WHERE east west} against other searchers like NSGAII and differential evolution algorithms\cite{storn1997differential}. This is to validate teh fact that the model doesn't only work for GALE style of perturbation. In Table \ref{external_validity}, we see that the models developed is valid for all searchers, as all searchers are able to find the near optimal solutions.

\begin{table}
\resizebox{3.3 in}{!}{


\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Searcher} & \multicolumn{2}{l|}{Apache} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Berkeley \\ DB C\end{tabular}} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Berkeley \\ DB Java\end{tabular}} & \multicolumn{2}{l|}{LLVM} & \multicolumn{2}{l|}{SQL} & \multicolumn{2}{l|}{X264} \\ \cline{2-13} 
                          & Median         & IQR        & Median                                 & IQR                                  & Median                                   & IQR                                   & Median       & IQR        & Median      & IQR        & Median       & IQR        \\ \hline
GALE                      & 870            & 0          & 0.3633                                 & 0.004                                & 3139                                     & 70                                    & 202.23       & 3.98       & 13.1284     & 0.2411     & 247.717      & 3.34       \\ \hline
DE                        & 840            & 0          & 0.3589                                 & 0.002                                & 3139                                     & 70                                    & 199.68       & 0          & 13.079      & 0          & 244.27       & 0.003      \\ \hline
NSGA2                     & 840            & 0          & 0.3536                                 & 0.005                                & 3139                                     & 70                                    & 199.68       & 0          & 13.079      & 0.406      & 244.27       & 0.05       \\ \hline
\end{tabular}}
\caption{The minimum performance scores as found by learners GALE, NSGAII and DE. The scores here show that all the learners are able to find configuration with minimum scores}
\label{external_validity}

\end{table}


{\em Parameter Bias} 
For this study, we did not do extensive parameter tuning:
NSGA-II and DE were run using their default
settings while GALE was run using the settings that
worked well on the first model we studied, which were
then frozen for the rest of this study. As documented
above, those parameters were:
\begin{itemize}
\item $\mu$ = 100: population size;
\item $\omega$ = $\mu$: minimum size leaf clusters;
\item $\lambda$ = 3: premature stopping criteria (sets the maximum
allowed generations without any improvement
on any objective).
\item $\delta$ = 1: the ``accelerator'' that encourages larger
mutations;
\item $\gamma$ = 1.5: the ``brake'' that blocks excessive mutation.
\end{itemize}

If this paper was arguing that these parameters were
somehow optimal, then it would be required to present
experiments defending the above settings. However, our
claim is less than that—we only aim to show that with
these settings, GALE does as well than standard searching
tools. In future work, we will explore other settings.




That said, there exist some class of data mining papers for which
tuning may not be required. Consider  Le Goues et al.'s 2012
ICSE paper that used a evolutionary program to learn
repairs to code~
in that paper was ``can we fix any of the known bugs?''. Note
that this criteria is a ``{\em competency}'' statement, and
not a ``{\em better than}'' statement (the difference being that
one is 
``can do'' and the other is ``can do better''). For such
competency claims, tuning is not necessary. However, as soon
as {\em better than} enters the performance criteria then this
becomes a race between competing methods. In such a race,
it is unfair to hobble one competitor with poor tunings.



\section{Conclusions}

We introduce three new sampling techniques called \textit{WHERE-east-west}, \textit{WHERE random} and \textit{WHERE exemplar} for performance-prediction of configurable systems. To evaluate and compare the techniques, we used prediction accuracy, run times and number of evaluations. We conducted empirical studies on six real-world configurable systems to determine an ideal sampling strategy for performance prediction of configurable software system. Our key findings are:
\bi
    \item{Clustering can also be used as a part of the process of sampling and can reduce the number of examples required to train a model}
    \item{Less data does help to build stable models since the variance is always lower than 6\%}
    \item{The model is stable enough so that any optimizer can use the model to find optimal configuration.}
    \item{Using extreme points of the first principle component is a good representative of the entire cluster.}
\ei

Our empirical findings are meant to help the stakeholders in finding optimal configurations to achieve the desired performance scores. Our approach has the following desirable properties:
\bi
\item{minimize the number of configurations which needs to be run. With systems with lot of configurable parameter out method can be very useful}
\item{flexible, since it can used with any number of configurations}
\ei
In the future...

\section{Future Work}

TO BE FILLED
 
 

\section*{Acknowledgments}
TO BE FILLED
 
\vspace*{0.5mm}
 
 
\bibliographystyle{plain}

\balance
\bibliography{activeconfig}  

   



  


  

\end{document}
 
\subsection{Implications}

time for an end to era of data mining in se? moving on to a new phase of learning-as-optimization

1) learning is actually an optimization tasks (e.g. see fig2 of  learners climbing the roc curve hill in http://goo.gl/x2EaAm)

2) our learners are all contorted to do some tasks X (e.g. minimize expected value of entropy), then we assess them on score Y (recall). which is nuts. maybe we should build the goal predicate into the learner (e.g http://menzies.us/pdf/10which.pdf) 

3) given 1 + 2, maybe the whole paradigm of optimizing param selection is wrong. maybe what we need is a library of bees buzzing around making random choices (e.g. about descritziation) which other bees use, plus their own random choices (e.g. max depth of tree learned from discretized data) which is used by other bees, plus their own random choices (e.g. business users reading the models).  the funky thing here is that it can take some time before some of the bees (the discretizers) get feedback from the community of people using their decision (the tree learners). 




