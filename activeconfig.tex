\documentclass{sig-alternative}
% \documentclass[conference]{IEEEtran}
\usepackage{color}
\usepackage{listings}
\usepackage{graphicx} 
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{cite}
\usepackage{paralist}
\usepackage[table,xcdraw]{xcolor}
\usepackage{siunitx}
\usepackage{rotating}
\usepackage{eqparbox}
\usepackage{todonotes}


\usepackage[framemethod=tikz]{mdframed}
\usepackage{lipsum}
\usetikzlibrary{shadows}
\newmdenv[tikzsetting= {fill=white!20},roundcorner=10pt, shadow=true]{myshadowbox}




\usepackage{graphics}
\usepackage{colortbl} 
\usepackage{multirow}
\usepackage{mathptmx} \usepackage[scaled=.90]{helvet} \usepackage{courier}
\usepackage{balance}
\usepackage{picture}
% \usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{soul}

\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}

\renewcommand\theadalign{cb}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
\renewcommand{\footnotesize}{\scriptsize}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[table]{xcolor}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\lstset{
    language=Python,
    basicstyle=\ttfamily\fontsize{2.7mm}{0.8em}\selectfont,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=l,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\bfseries,
    emph={furthest,gale,better,improved,where,fastmap,split,project,mutate,mutate1}, emphstyle=\bfseries\color{blue},
    stringstyle=\color{green!50!black},
    commentstyle=\color{gray}\itshape,
    numbers=left,
    captionpos=t,
    escapeinside={\%*}{*)}
}


%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}
%\newcommand{\rone}{\crule{1mm}{1.95mm}}
%\newcommand{\rtwo}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
%\newcommand{\rthree}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
%\newcommand{\rfour}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}} 
%\newcommand{\rfive}{\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}\hspace{0.3pt}\crule{1mm}{1.95mm}}
\newcommand{\quart}[3]{\begin{picture}(100,6)%1
{\color{black}\put(#3,3){\circle*{4}}\put(#1,3){\line(1,0){#2}}}\end{picture}}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}
% \newcommand{\rone}{}
% \newcommand{\rtwo}{}
% \newcommand{\rthree}{}
% \newcommand{\rfour}{} 
% \newcommand{\rfive}{}
%% timm tricks
\newcommand{\bi}{\begin{itemize}}%[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table ~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\what}{{\bf WHAT }}

%% space saving measures

%\usepackage[shortlabels]{enumitem}  
\usepackage{url}
% \def\baselinestretch{1}


% \setlist{nosep}
%  \usepackage[font={small}]{caption, subfig}
% \setlength{\abovecaptionskip}{1ex}
%  \setlength{\belowcaptionskip}{1ex}

%  \setlength{\floatsep}{1ex}
%  \setlength{\textfloatsep}{1ex}
%  \newcommand{\subparagraph}{}

% \usepackage[compact,small]{titlesec}
% \DeclareMathSizes{7}{7}{7}{7} 
% \setlength{\columnsep}{7mm}


% \usepackage[svgnames]{xcolor}
% \usepackage[framed]{ntheorem}
% \usepackage{framed}
% \usepackage{tikz}
% \usetikzlibrary{shadows}
% %\newtheorem{Lesson}{Lesson}
% \theoremclass{Lesson}
% \theoremstyle{break}


 
\begin{document}
% \conferenceinfo{FSE}{'15 Bergamo, Italy}
\title{Faster Discovery of Faster System Configurations with Spectral Learning} 
\numberofauthors{2}
\author{
        \alignauthor Vivek Nair, Tim Menzies, Xipeng Shen 
        \affaddr{NC State University, USA}\\
        \email{{vivekaxl,tim.menzies, xipengshen@gmail.com}}
    \and  
        \alignauthor Norbert Siegmund, Sven Apel \\
        \affaddr{University of Passau, Germany}\\
        \email{{norbert.siegmund, apel@uni-passau.de}}
       }
\maketitle 
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}
Prior work on predicting the performance of software configurations suffered from either (a)~requiring far too many sample configurations or (b)~large variances in their predictions.
Both these problems can be avoided using the \what spectral learner.  
{\what}'s innovation is  
the use of the spectrum (eigenvalues) of the distance matrix
between the configurations of a configurable system, to perform dimensionality reduction. Within that
reduced configuration space, many closely associated configurations can be studied
by executing only a few sample configurations. For the systems studied
here, a few dozen samples yield accurate and stable predictors---less than 10\,\% prediction error, with a standard deviation of less than 2\,\%.  
When compared to the state of the art, our approach (a)~requires 
2 to 10 times fewer samples to achieve similar error rates
and (b)~its predictions are  more accurate (i.e., have lower standard
deviation). 
%unclear: (i.e., much mean errors and standard deviations on the errors, refer to Figure 9-11)
Furthermore, predictive models generated by
\what, along with the sampling techniques we propose, can be used by optimizers to discover system configurations that closely approach the optimal performance, as we will demonstrate.
\end{abstract}

% A category with the (minimum) three required fields
\vspace{1mm}
\noindent
{\bf Categories/Subject Descriptors:} 
D.2 [Software Engineering];
I.2 [Artificial Intelligence];

 
%\vspace{1mm}
\noindent
{\bf Keywords:} Performance Prediction, 
Spectral Learning, 
Decision Trees,
Search-Based Software Engineering, 
Sampling.
\pagenumbering{arabic} %XXX delete before submission
 
\begin{comment} 
\end{comment}
 
\section{Introduction}
 
%==Are these comments?== 
%Are feature/variability models too abstract for concrete reasoning?
%Or is it possible to use these models to predict specific properties 
%of programs generated from them? 

Most software systems today are configurable. Despite the undeniable benefits
of configurability, large configuration spaces challenge developers, maintainers, and users. In the face of hundreds of configuration options, it is not easy to keep track of the effects of individual configuration options and their mutual interactions. Predicting the performance of individual system configurations or determining the optimal configuration is often more guess work than engineering. In their recent paper, Xu et al.\ documented the  difficulties developers face
with understanding  the configuration spaces of their systems~\cite{xu2015hey}. As a result, developers tend to ignore over $5/6$ths of the configuration options, which leaves considerable optimization potential untapped and induces major economic cost~\cite{xu2015hey}.

Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning~\cite{siegmund2012predicting,guo2013variability,sarkar2015cost}. While gaining some ground, state-of-the-art approaches face two problems: 
(a)~they require far too many samples configurations for learning or (b)~they are prone to large variances in their predictions. For example, prior work on predicting performance scores using regression trees had to compile and execute hundreds to thousands of specific system configurations~\cite{guo2013variability}. 
Other studies that tried to make far fewer execution samples resulted in predictors that were considerably inaccurate in their performance predictions.\todo[inline]{Which do you mean?}
A more balanced approach by Siegmund et al.\ is able to learn predictors for  configurable systems~\cite{siegmund2012predicting} with low mean errors, but with large variances of prediction accuracy  (e.g.\ in half of the results, the predictions for  Apache  were up to 50\,\% wrong). 
%My(Norbert) comment on this: you can't just take the worst heuristic here when the paper was about improving over this worst case...
 %\footnote{The predictions of \cite{siegmund2012predicting} had $\mu=19\%$ errors with $\sigma = 19.5\%$.}
  %that half they time, they could be up to 50\% incorrect\footnote{
 %The range  $\mu \pm 1.5*\sigma$  includes the
 %25th to 75th percentile range of a normal distribution   
%For the \cite{siegmund2012predicting} errors, that range is  0 to 49\%.}. 
% >> Where do you have this data from? Does not match with the paper.
% Vivek: You take the fault rate and standard deviation of the method which uses **almost** same  number of measurements. 
Guo et al.~\cite{guo2013variability} also proposed an incremental method to build a predictor model, which uses incremental random samples with steps equal to the number of configuration options (features) of the system. This approach also
suffered from  unstable predictions (e.g., predictions had a mean error of up to 22\,\%, with a standard deviation of up 46\,\%). Sarkar et al.~\cite{sarkar2015cost} proposed a proj\-ective-learning approach (using fewer measurements than Guo at al.\ and Siegmund et al.) to quickly compute  the number of configurations to measure for learning a stable predictor. However, as we will discuss, after making that prediction, the total number of samples required for learning the predictor is considerable, possibly too large for practical use (up to hundreds of samples).

The problems of large sample sets and large variances in prediction can be avoided using the \what spectral learner.  
{\what}'s innovation is  the use of the spectrum (eigenvalues) of the distance matrix
between the configurations of a configurable system, to perform dimensionality reduction. Within that
reduced configuration space, many closely associated configurations can be studied
by measuring only a few samples.
We have compared \what against the state-of-the-art approaches of Siegmund et al.~\cite{siegmund2012predicting}, Guo et al.~\cite{guo2013variability}, and Sarkar et al. \cite{sarkar2015cost} by means of 5 real-world configurable systems: Berkeley DB,  the Apache Web server, SQLite, the LLVM compiler, and the x264 video encoder.
We found that \what performs as well or better than prior approaches,
while  requiring far few samples (just a few dozen).
This is significant and most surprising.  since some of the systems explored here have up to millions of possible variations. 

The specific contributions of this paper are:
\bi
\item We present a novel sampling and learning approach for predicting the performance of software configurations. The approach is based on a
{\em spectral
learner} that uses an approximation to the first principal component of the configuration space to recursively cluster it, relying only on a few points as a representative of each cluster;
\item We demonstrate the practicality and generality of our approach by conducting experiments on six real-world configurable software systems (see Figure ~\ref{fig:systems}). The results show that our approach is more accurate (lower mean error) and more stable (lower standard deviation) than state-of-the-art approaches;
\item We report on a comparative analysis of our approach and three state-of-the-art approaches, demonstrating that our approach outperforms previous approaches in terms of sample size and prediction stability. A key finding is the utility of the principal component of a configuration space to  find informative samples from an otherwise large configuration space.
\ei
More generally, this paper illustrates the value of spectral learning for software-engineering data sets. We recommend spectral learning for any software analytics
tasks with many possible options described by sets of attributes
that might be noisy or redundant. For further details on why we make this recommendation, see \tion{sample}.

%The rest of the paper is organized as follows. Section 2 defines the terms used in the paper. Section 3 describes the background of the problem we are trying to tackle along with the data sets used in the experiments. Section 4 further explores the research question and finds association among them. Section 5 describes the experiment setup etc. Section 6 discusses the results and Section 7 the related work. This paper concludes after mentioning the threats to validity.


\section{Background \& Related Work}  

\subsection{Predicting Performance}\label{sect:addit}

A configurable software system has a set $X$ of Boolean configuration options, also referred to as features or independent variables. If a feature is selected in a configuration, it corresponds to $True$ and $False$ otherwise. We represent the features as a vector F=  $<f_1$,$f_2$, ..., $f_N>$, where $N$ is the number of features of the system. Each valid instance of the vector (i.e., a configuration) has a corresponding performance score associated to it. 

The literature offers two approaches to performance prediction of software configurations: a {\em maximal sampling} and a {\em minimal sampling} approach: 
With {\em maximal sampling}, we compile all  possible configurations and record the associated performance scores. 
Maximal sampling  can be impractically slow. For example, the performance data used in this paper required  26 days of CPU time to collect (and much longer, if we also count the time required for compiling the code prior to execution). 
 Other researchers have commented that,  in 
 real world scenarios, the cost of acquiring optimal configuration is overly expensive and time consuming \cite{weiss2008maximizing}.
 
 If collecting performance scores on all configurations is impractical,  {\em minimal sampling} 
 is used to intelligently select and execute just enough configurations (i.e., samples) to build a
 predictive model.
 For example, Zhang et al.~\cite{zhang2015performance} approximate the
 systems as a Fourier series, after which they can derive an expression showing how many configurations must be studied
 to build predictive models with error $\epsilon$. While a theoretically satisfying result, that approach still needs thousands to hundreds of thousands of executions of sample
 configurations.  

Another set of approaches are the four "additive" {\em minimal sampling} methods for Siegmund et al.~\cite{siegmund2012predicting}.
Their first method, called feature-wise ({\em FW}), is their basic method and 
three of these methods are  elaborations of their basic method.
To explain FW, we note that, from a configurable software system, it is theoretically possible to enumerate many or all of the legal configurations\footnote{Though, in practice, this can be very difficult. For example, in models like the Linux Kernel such an enumeration is practically impossible ~\cite{sayyad13b}.}. 
Each such
configuration is a vector of $n$ Booleans where $n_i$ indicates if this configuration uses feature $f_i$.
Using this information it  is possible to isolate examples of how much each feature individually contributes to the total run time:
\be
\item Find a pair of  configurations $C_1,C_2$  where $C_2$ uses exactly the same features as $C_1$, plus just one  extra feature $f$.
\item Set the run time $\Pi(f)$ for feature $f$ to be the difference in the performance scores between $C_2$ and $C_1$.
\item The run time  for a new configuration  $C_i=\{f_1,f_2,f_3, ...\}$, that has not been sampled before,  is the sum of the run time of its features, i.e.,
\begin{equation}
  \Pi(C_i) = \sum_{f_j \in C_i}\Pi(f_j)  
\end{equation}
\ee

Further to the first point, when many pairs such as ${C_1,C_2}$ satisfying the criteria of point~1, Siegmund et al.\ used the 
pair that mentions the {\em smallest} number of features. Their basic {\em FW} minimal sampling method 
just compiles and executes just these smallest $C_1,C_2$ configurations. 

Siegmund et al.\ also offered three extensions to the basic method. All these extensions are based on sampling
not just the smallest $C_1,C_2$  pairs but also any configurations with {\em interactions}. 
All the following minimal sampling policies compile and   execute legal configurations selected via one of three heuristics:
\bi
\item[{\em PW (pair-wise):}] For each pair of features, try to find a configuration that contains the pair and has a minimal number of features selected. 
\item[{\em HO (higher-order):}] Select extra configurations, in which three features $a,b,c$ are selected if two of the following know pair-wise interactions exist $(a,b)$ and $(b,c)$ and $(a,c)$.
\item[{\em HS (hot-spot features):}] Select extra configurations that contain features that are
frequently interacting with other features. 
\ei



Guo et al.~\cite{guo2013variability} proposed a progressive random sampling methodology, which samples in steps of the number of features of the software system in question. The sampled configurations were used to train a regression tree and which is then used to predict the performance scores of other system configurations. The termination criterion of this technique is based on a heuristic, similar to the {\em PW} heurstics of Siegmund et al. 

Sarkar et al.~\cite{sarkar2015cost} proposed a cost model for predicting the effort (or cost) required to generate an accurate predictive model. The user can use this score to decide whether to go ahead and build the model. The method proposed by Sarkar et al.\ randomly samples configurations and uses a heuristic based on feature frequencies as a termination criterion. These samples are then used to train a regression tree, and the accuracy of the model is measured by using a test set (size of train set is equal to size of the test set). One of the four projective functions is selected based on how correlated they are to  accuracy measures. This projective function is used to approximate the accuracy-measure curve. The elbow point of the curve is then used as the optimal sample size. Once the optimal size is known, Sarkar et al.\ uses the model proposed by Guo et al.\ to build the model.  


The advantage of these methods are that, unlike  the results of Zhang et al., these  methods require only dozens to hundreds of samples. Also, unlike our approach, these sampling heuristics do not require to enumerate all configurations which is important for highly configurable software systems. 
That said, as shown by our experiments, these approaches produce estimates with  larger mean error sand partially larger variances than our approach. While sometimes the Sarkar's approach results in  models with (slightly)
lower mean fault rates, it still requires a considerably largeer number of samples (up to hundreds), while \what only requires to evaluate a  few dozen.
 

\subsection{Spectral Learning}\label{sect:spect}

The minimal sampling method explored in this paper is based on a spectral learning algorithm
that  explores the spectrum (eigenvalues) of the distance matrix between  configurations.
In theory, such spectral learners are a better way to handle noisy, redundant, and tightly inter-connected variables, for the following reasons.
When data sets have many irrelevancies or closely associated data parameters $d$, then
only a few $e \ll d$ eigenvectors are required to characterize that data.
In that reduced space:
\bi
\item
Multiple inter-connected variables $i,j,k \subseteq d$ can be represented
by a single eigenvector.
\item
Noisy variables from $d$ are
ignored, because they  do not contribute to the signal in the data.
\item
Variables  become (approximately) parallel lines
in $e$ space. For  redundancies \mbox{$i,j \in d$}, we
can ignore $j$
since effects that change over $j$ also
change in the same way over $i$.
\ei
That is, in theory, samples of configurations drawn via an eigenspace sampling method
would not get confused by noisy, redundant, or tightly inter-connected variables. Accordingly,
we expect predictions built from that sample to have  lower mean errors and lower variances on that error.

Spectral methods have been used before for a variety of data mining applications~\cite{kamvar2003spectral}.
Algorithms, such as PDDP~\cite{boley98} use spectral methods, such as principle component analysis (PCA) to
recursively divide data into smaller regions.  Software analytics researchers use spectral methods (again, PCA) as a pre-processor prior to data mining  to reduce noise in software-related data sets~\cite{Theisen15}.
However, to the best of our knowledge, spectral methods have not been used before in software engineering as a basis of a minimal sampling policy.



\what is somewhat different from other spectral
learners explored in, for instance, image processing applications~\cite{shi00}.
The image processing papers do not address
defining a minimal sampling policy to predict performance scores.
Also, standard spectral method requires some $O(N^2)$ matrix multiplication to compute the components
of PCA~\cite{ilin10}. Worse, in the case of hierarchical division methods such as PDDP,
that polynomial time inference must be repeated at every level of the hierarchy.
Competitive results can be achieved
using an $O(2N)$ analysis that we have developed previously~\cite{me12d}, which is  based on  a heuristic proposed by Faloutsos and Lin~\cite{Faloutsos1995} (which Platt has shown computes a Nystr\"om approximation to the first component of PCA~\cite{platt05}).
  
Our approach inputs $n$
examples $n_1,n_2,..$ then:
\be
\item
Picks any
point $n_i$ at random;
\item
Finds
 the point  {\em West}~$\in n$ that is
furthest away from $n_i$;
\item Finds the point {\em East}~$\in n$
that is furthest from {\em West}.
\ee
Using the distance calculation shown in Equation~\2, 
see define $c$ to be the distance betewen {\em East}
and {\em West}.
\what uses this distance to divide all the configurations as follows.

\[
\forall  n_i \in n \left\{\begin{array}{rcl}
a&=&\mathit{dist}(n_i,\mathit{West})\\
b&=&\mathit{dist}(n_i,\mathit{East})\\
x_i&=&\frac{a^2 + c^2 - b^2}{2c}
\end{array}
\]
This  $x_i$ value is the projection of $n_i$
on the line  running  from {\em East} to {\em West}.  We divide
the examples based on the median value of the projection($x_i$),
then recurse on each half. The recursion on
$n$ initial
examples stops when a sub-region
contains less that  $M$ examples (e.g. 
$M=\sqrt{|n|}$).
We explore this approach for three reasons:
\bi
\item
{\em It is very fast}:
This process requires only $2*|n|$ distance comparisons
per level of recursion, which is far less than the $O(|n|^2)$
required by PCA~\cite{Du2008}
or other  algorithms such as K-Means~\cite{hamerly2010making}.
\item
{\em It is not domain specific}:
This approach is general unlike traditional PCA. It does not make any assumption that all the variables are numeric. As shown in equation 2, we can approximate distances for both numeric and non-numeric data.

\begin{equation}
    dist(x, y) = 
    \begin{cases}
        \sqrt{\sum_i(x_i-y_i)^2},& \text{if $x_i$ and $y_i$ is numeric}\\
        \begin{cases}
            0, & \text{ if $x_i$ == $y_i$}\\
            1, & \text{ otherwise}\\
        \end{cases}
        ,& \text{if $x_i$ and $y_i$ is boolean}\\
    \end{cases}
\end{equation}

\item
{\em Explore the underlying dimension of the search space (configuration space)}:
This technique explores the underlying dimension (first principal component) without getting confused by noisy, related and highly associated variables.

\ei

\subsection{Spectral Sampling}\label{sect:sample}
When the above clustering method terminates, our  sampling policy (which we will call $S_1$:Random) is then applied:
\begin{quote}
$S_1$: Random sampling: Compile and execute one  configurations,  picked at random, from each leaf cluster.
\end{quote}
We use this sampling policy, because (as we will show later) $S_1$:Random sampling performs better than:
\bi
\item $S_2$: East-West sampling: compile and execute the {\em East} and {\em West} poles of the leaf clusters;
\item $S_3$: Exemplar sampling: compile and execute all items in all leaves and return the one
with lowest performance score.
\ei
Note that $S_3$ is {\em not} a {\em minimal} sampling policy (since it executes all configurations). 
We use it here as one  baseline
against which we can compare the other, more minimal, sampling policies. In the results
that follow, we also compare our 
sampling methods against another baseline using information gathered after executing
all configurations.

\subsection{Regression Tree Learning}
After collecting the data using one of the sampling policies, we then use  CART regression tree learner \cite{breiman1984} to build a predictor for performance scores. Regression tree learners seek the attribute range split that most increases
our ability to make accurate predictions.
CART explores splits which divide $n$ samples  into  $n_1$ and $n_2$ samples, where each sample  has a  standard deviation on the target variable of $\sigma_1$ and  $\sigma_2$.
CART finds the ``best'' split is defined as the split which minimizes $\frac{n_1}{n}\sigma_1 + \frac{n_2}{n}\sigma_2$.
Using this best split, CART divides the data recursively.
 

The validity of the predictors built by CART  is then tested on testing data. There is no intersection between the training data (spectral learning processing) and the testing data (see below for details). 
For each  test item, we find out how long it {\em actually} takes to run the corresponding variant and compare the actual measured performance to the {\em prediction} from CART. The resulting prediction error is then computed using:
\begin{equation}\label{eq:err}
\mathit{error}=\frac{\mathit{abs}(\mathit{predicted} - \mathit{actual})}{\mathit{actual}}*100
\end{equation}

\section{Research Questions} 
In summary, \what  combines:
\be
\item
The FASTMAP method of Faloutsos and Lin~\cite{Faloutsos1995};
\item A spectral learning algorithm initially   inspired by    Boley's PDDP system~\cite{boley98}, which we modify
by replacing  PCA with FASTMAP (called
``WHERE'' in prior work ~\cite{me12d});
\item
The $S_1$:Random sample method that explores the leaf clusters found by this recursive division;
\item 
The CART regression tree learner that converts the data from the samples collected by $S_1$:Random
into a runtime prediction model~\cite{breiman1984}.
\ee
That is,
\begin{center}
\begin{tabular}{rcl}
WHERE& = &PDDP - PCA + FASTMAP\\ 
\what& =  & WHERE + $S_1$:Random + CART
\end{tabular}
\end{center}
This unique combination of methods has not been previously explored in the
software-engineering literature but is promising because it explores the underlying dimension of the space without getting confused by noisy, redundant, and highly associated variables (see Section 3.3).

%Could be removed if space is needed
The rest of this section defines four research questions. To
define those questions, we list below all the things that might go
wrong when a large complex space is explored via \what.

Since our model explores the spectral space, it might be true that only a small
number of samples are required to explore the whole space.
However, a model built from a very small sample of the available data might
be very inaccurate and unstable i.e. it may exhibit very large mean errors and standard deviations on the error.

%Could be removed if space is needed
Also, if we learn models from small regions of the training data,
then it is  possible that that learner will miss {\em trends} in the data
between the sample points. Such trends are useful when building {\em optimizers};
i.e. devices that input one configuration and propose an alternate
configuration that has, for instance, faster performance scores. Such optimizers might
need to evaluate hundreds to millions of alternate configurations. 
To speed up that process, optimizers can use a {\em surrogate model}\footnote{Also known as response surface methods, meta models, or emulators.}
that  mimics the outputs of a system of interest, while being computationally cheap(er) to evaluate~\cite{loshchilov13}. For example, when optimizing
performance scores, we might ask the CART  for a performance
prediction (rather than compile and execute
configurations).  Note that such surrogate-based
reasoning critically depends on how well the surrogate can guide optimization.


Therefore, to assess our sampling policies, we must consider:
\begin{compactitem}
\item Performance scores generated from our minimal sampling policy;
\item The variance of the error rates when comparing predicted performance score with actual ones;
%the next point should always be true. otherwise, what would be the reason to do that?!
\item The optimization support offered by the performance predictor (i.e., can the model work in tandem with other off-the-shelf optimizers to generate useful solutions).
\end{compactitem}
The above concerns leads to four research questions:
\begin{compactitem}
\item {\bf RQ1:} {\em Can  \what  + $S_1$:Random spectral learning generate good predictions after
executing only a small number of configurations?}
\end{compactitem}
Here, by ``good'' we mean that the predictions found via sampling with \what + $S_1$:Random are as good, or better,
as those generated from more samples.
\bi
\item {\bf RQ2:} {\em
Does less data used in building the models lead to large variance in the predicted values?}
\item {\bf RQ3:} {\em
Can ``good'' surrogate models (to be used in optimizers)
be built from minimal samples?}
\ei
Note that {\bf RQ2, RQ3} are of particular concern with our approach,
since our goal is to sample as little as possible from the configuration space.
\bi
\item {\bf RQ4:} {\em Compared to the state-of-the-art in minimal sampling for
learning performance-score predictors from configurable software systems, how good is \what with $S_1$:Random?}
\ei
To answer this last question, we will compare \what with $S_1$:Random
          against approaches presented in ~\cite{siegmund2012predicting}, \cite{guo2013variability}, \cite{sarkar2015cost}.
 
\section{Experiments}

%\subsection{Reproduction Package}

All the materials required to reproduce this work are available at \url{https://goo.gl/689Dve}.

\subsection{Case Study Materials}

The configurable systems used in our experiments are described in Figure \ref{fig:systems}.
Note, with ``predicting performance scores'', we 
mean predicting performance measures of the system while executing the test suite provided by the manufacturers, as described per subject system.
Also, to compare our results from minimal sub-sampling to those obtained 
from a larger sample, we use data sets showing the performance scores associated with
compiling and executing {\em nearly all} configurations\footnote{http://openscience.us/repo/performance-predict/cpm.html}.
\todo[inline]{Sorry, I do not understand the above sentence.}
We say {\em nearly all} configurations, for the following reasoning: For 
$\frac{5}{6}$ths of our case studies, the number of possible configurations
was not large (192 to 2560). However, the SQLite product line has 3,932,160 
possible configurations-- which is an impractically large number of configurations to explore. Hence, for SQLite, we use the 4500 samples that could
be collected in one day of CPU time (and we will pay particular attention to the variance of the SQLite results).



\begin{figure}[!t]\small
\framebox[\columnwidth][c]{
\begin{tabular}{lrrr}
\multicolumn{4}{p{.95\linewidth}}{
\textbf{Berkeley DB CE} is an embedded database system written in C. It is one of the most deployed databases in the world, due to its low binary footprint and its configuration abilities. We used the benchmark provided by the vendor to measure response time.}\\
%
\multicolumn{4}{p{.95\linewidth}}{
\textbf{Berkeley DB JE} is a complete re-development in Java with full SQL support. Similarly, we used a benchmark provided by the vendor measuring response time.}\\
%
\multicolumn{4}{p{.95\linewidth}}{
\textbf{Apache} is a prominent open-source Web server that comes with various configuration options. To measure performance, we used the tools autobench and httperf to generate load on the Web server. We increased the load until the server could not handle any further requests and marked the maximum load as the performance value.}\\
%
\multicolumn{4}{p{.95\linewidth}}{
\textbf{SQLite} is an embedded database system deployed over several millions of devices. It supports a vast number of configuration options in terms of compiler flags. As benchmark, we used the benchmark provided by the vendor and measured the response time.}\\
%
\multicolumn{4}{p{.95\linewidth}}{
\textbf{LLVM} is a compiler infrastructure written in C++. It provides configuration options to tailor the compilation process. As benchmark, we measured the time to compile LLVM's test suite.}\\
%
\multicolumn{4}{p{.95\linewidth}}{
\textbf{x264} is a video encoder in C that provides configuration options to adjust output quality of encoded video files. As a benchmark, we encoded the Sintel trailer (735\%MB) from avi to the xH.264 codec and measured encoding time.}\\[4ex]
\hline
System & LOC & Features & Configurations\\\hline
Berkeley DB CE   & 219,811 & 18 & 2,560\\
Berkeley DB JE   & 42,596 & 32  & 400\\
Apache & 230,277 & 9 & 192\\
SQLite & 312,625 & 39 & 3,932,160\\
LLVM & 47,549 & 11 & 1,024\\
x264 & 45,743 & 16 & 1,152\\\hline
\end{tabular}
}
\caption{Subject systems used in the experiments.}\label{fig:systems}
\end{figure}




\subsection{Experimental Rig}


{\bf RQ1} and {\bf RQ2} require the construction and assessment of numerous runtime predictors from small samples
of the data. The following rig implements that construction process.

For each configurable software system, we built a table of data, one row per valid configuration. We then run all configurations of all software systems
and record the performance scores (i.e., that are invoked by a benchmark).
The exception is SQLite in which we measure only the
configurations needed to detect interactions and additionally
100 random configurations to evaluate the accuracy of
predictions.  
To this table, we added a column showing the performance score obtained from the actual measurements for each configuration (refer to section 2).

Next, 20 times, we repeated the following procedure (the figure of 20 repeats was
selected using the central limit theorem). Note that the following rig ensures that
we \textbf{never} test any prediction model on the data used to learn that model.
\bi
\item For each system in \{BDBC, BDBJ, Apache, SQLite, LLVM, x264\},
\bi
\item Randomize the order of the rows in their table of data;
\item For $X$ in \{10, 20, 30, ... , 90\};
\bi
\item Let {\em Train} be the first $X$\,\% of the data 
\item Let {\em Test} be the rest of the data;
\item Pass {\em Train} to \what to find a sample of configurations;
\item Determine the performance scores associated with those configurations (in our rig, this corresponds to a table lookup, but would entail compiling and executing the software variant in a practical setting).
\item Using the {\em Train}  data and their performance scores, build a performance predictor using CART.
\item Using {\em Test}, assess the accuracy of the predictor using the error 
measure of \eq{err}.
\ei
\ei
\ei
{\bf RQ2} requires testing the standard deviation of the predictions. To support that test, we:
\bi
\item Determine the $X$-th point in the above experiments where all predictions stop improving (elbow point);
\item Measure the standard deviation of the error at that point, across our 20 repeats.
\ei
As shown in figure \ref{fig:sampling_accuracy}, all our results plateaued after studying $X=40$\,\% of the valid configurations
 \footnote{Just to clarify one frequently asked question about this work, we note
that our rig ``studies'' 40\,\% of the data. We do not mean that our predictive models
 require accessing the performance scores from the 40\,\% of the data. Rather, by ``study'' we mean   reflect 
 on a sample of configurations to determine what minimal subset of that
sample deserves to be compiled and executed.}.
 Hence to answer {\bf RQ2}, we will compare all 20 predictions at $X=40$\,\%.
 
{\bf RQ3} requires using the regression tree learned in this way as a {\em surrogate model} within an optimization process. To study this research question, we:
\bi
\item Take   $X=40\,\%$ of the configurations;
\item Apply \what with $S_1$:Random to build a CART model using some minimal sample taken from that 40\,\%;
\item Used that CART model within some standard optimizers while searching for 
configurations with least runtime;
\item  Compare the faster configurations found in this manner with the fastest configuration
known for that system.
\ei
This last item requires access to some ``ground truth'' of performance scores for a very
large number of configurations. For this experiment, we have access to that ground truth
(since we have access to nearly all the configurations). While that ground truth is required for this
validation study, we note that, once our methods satisfy this validation study,
they would not not be needed if practitioners choose to use \what in their own work.

As to the optimizers used in this second experiment, for the sake of completeness, we explored
a range of optimizers seen in the   literature:  DE~\cite{storn1997differential}, NSGA-II~\cite{deb00afast}
and our own GALE~\cite{krall2014gale,zuluaga2013active} system.   Normally,
it would be  reasonable to ask
why we used those three, and not the hundreds of other 
optimizers described in the literature~\cite{fletcher13,harman12}. However,
as shown below, all these optimizers in this
domain exhibited  very similar
behavior (all found configurations close to the
best case performance). Hence, the specific
choice of optimizer is not a critical
variable in  our analysis.


\newpage
\section{Results}


\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/SamplingAccuracy.eps}
\caption{Error seen in the predictions made by \what with four different
sampling policies. Note that. on the y-axis,  {\em lower} errors are {\em better}.}
\label{fig:sampling_accuracy}
\end{figure}

\subsection{RQ1}
{\em Can  \what  + $S_1$:Random spectral learning generate good predictions after
executing only a small number of configurations?}

\fig{sampling_accuracy} shows the mean errors of the predictors learned
after taking $X$\,\% of the configurations, then asking  \what and some sampling method
to (a)~find what configurations to measure; then (b)~asking CART to build a predictor
using that execution information. The horizontal axis of those plots shows what $X$\,\%
of the configurations are studied and the vertical axis shows the mean relative error (from \eq{err}).
In that figure:
\begin{compactitem}
\item
The \textcolor{blue}{{\bf blue}} lines on \fig{sampling_accuracy} show a {\em baseline} result
where data from the performance scores of 100\,\% of  configurations were used by CART
to build a runtime predictor.
\item
The other lines show the results using the sampling methods defined in \tion{sample}.
Note that these sampling methods used  runtime data only from a
subset of 100\,\% of the performance scores seen in configurations
from 0 to X\,\%.
\end{compactitem}


The {\em lower} y-axis values  in  \fig{sampling_accuracy} are {\em better} since this shows lower
prediction errors. We found that:
\begin{compactitem}

\item Some software systems exhibit large variances in their error rate, below $X=40$\,\% (e.g., BDBC and BDBJ).
\item Above $X=40$\,\%, there is little effect on other overall change of the sampling methods.
\item
In most of the experiments, the {\em exempler} sampling method $S_3$ shows the highest overall error 
so that it cannot be recommended.
\item Always, the   blue baseline shows the lowest errors, which is to be
expected since predictors built on the baseline have access to all data.
\item
We see a trend that the error of  $S_1$:Random and $S_2$:EastWest are within $5$\,\% of the {\em baseline} results.
Hence, we can recommend these two minimal sampling methods.
\end{compactitem}

\fig{Evaluations} comments on one which  of    $S_1$:Random or $S_2$:EastWest we should recommend.
This figure displays data taken from the $X=40$\,\% point of \fig{sampling_accuracy} and displays
how many performance scores of configurations are needed by our sub-sampling methods (while
reflecting on the configurations seen in the range $0\le X \le 40$. Note that:
\begin{compactitem}
\item
The {\em Exemplar} sampling policy needs up to thousands of performance-score points, 
so it cannot be recommended as minimal sampling policy;
\item The {\em East-West} sampling policy needs twice as much performance-score information as 
the $S_1$:Random {\em random} policy (since $S_2$ uses {\em two} samples per leaf cluster  while
$S_1$:Random uses only {\em one}).
\item $S_1$:Random needs performance-score information on only a few dozen (or less) configurations to generate
the predictions with the lower errors seen in \fig{sampling_accuracy}.
\end{compactitem}
Combining \fig{sampling_accuracy} and \fig{Evaluations} results, we conclude that:

\begin{myshadowbox}
$S_1$:Random is our preferred spectral sampling method. Further,
the answer to {\bf RQ1} is ``yes'', because applying \what + $S_1$: Random, we can (a)~generate runtime predictors
using just a few dozens examples of performance scores; and (b)~those predictions have error rates
within 5\,\% of the error rates seen if predictors built from information about all performance scores.
\end{myshadowbox}

\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/evaluation_graph.eps}
\caption{ Comparing evaluations of different sampling policies. We see that the number of configurations evaluated for $S_2$:EastWest is twice as much as $S_1$:Random, since it selects 2 points from each cluster where as  $S_1$:Random selects only one }\label{fig:Evaluations}
\end{figure}


\newpage\subsection{RQ2}

{\em
Does less data used in building the models lead to large variance in the predicted values?}\\

Two competing effects can lead to increased or decreased  variance  in 
runtime predictions.
The   less we sample the configuration space,
the less we constrain model generation in that space. Hence, one effect that might be expected
is that models learned
from too few samples can exhibit large variance. 
On the other hand,
a  compensating effect can be introduced by sampling from spectral space
since that space contains fewer confusing variables than that raw data.

\fig{Variance} reports which one of these two competing effects are dominant. 
The \fig{sampling_accuracy} shows that after some initial fluctuations,
after seeing $X=40$\,\% of the data, the variability in predictions reduces to nearly zero.


\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/Variance.eps}
\caption{Standard deviations seen at various points of  \fig{sampling_accuracy}.}\label{fig:Variance}
\end{figure}

\begin{myshadowbox}
Hence, we answer {\bf RQ2} with ``no'': selecting a small number of samples does not increase variance (at least to say, not in this domain).
\end{myshadowbox}




\newpage
\subsection{RQ3}

 {\em
Can ``good'' surrogate models (to be used in optimizers)
be built from minimal samples?}

The answers to {\bf RQ1} and {\bf RQ2} recommend using \what+$S_1$:Random to build runtime predictors from a small sample of the data. {\bf RQ3}
asks if that predictor can be used by an optimizer to infer what {\em other} configurations correspond to system variants with fast performance scores?
To answer this question,  we ran  a random set of 100 
configurations, 20 times, then relate that baseline to three optimizers (GALE~\cite{krall2014gale}, DE~\cite{storn1997differential}, NSGA-II~\cite{deb00afast}) using their
default parameters.
 
When these three optimizers mutated existing configurations to suggest new ones,
those mutations were checked against the feature model. Any mutants that violated the feature-model constraints were rejected
and the survivors were ``evaluated'' by asking the CART surrogate model.
%(the  CART regression tree learned from \what+$S_1$:Random, built using the methods of {\bf RQ1}).
These evaluations either rejected the mutant or used it in generation $i+1$, as the basis for a search for more, possibly
better  mutants.




\fig{performance_graph} shows the configurations found by GALE projected onto the ``ground truth'' of the performance scores of nearly
all configurations. Again note that, while we use that ``ground truth'' for the validation of these results, our optimizers 
used only a small percent of that ground truth data in their search for the fastest configurations (see the \what + $S_1$:Random
results of \fig{Evaluations}).


\begin{figure}[!t]
\includegraphics[width=0.9\linewidth]{Figures/optimizer_result.eps}
\caption{Solutions found by GALE and NSGA-II and DE (shown as points) laid against the ground truth (all known configuration performance scores). In the top plot, three results can clearly be distinguished. However, in the remaining plots, the results were very similar. Hence, in those plots, all those results
fall to very nearly the same point.}\label{fig:performance_graph}
\end{figure}


The important feature of \fig{performance_graph} is that all the optimized configurations fall within 1\,\% of the fastest
configuration in the ground truth (see all the left-hand-side dots on each plot). \fig{external_validity} compares GALE's performance to that of the two other optimizers
used in this study. Note that the performances are nearly identical which leads to the following conclusions:

\begin{myshadowbox}
The answer to {\bf RQ3} is ``yes''--- for optimizing performance scores, we can use surrogates built from few runtime samples. Also, the choice of the optimizer does not critically effect this conclusion.
\end{myshadowbox}


\begin{figure}[h]
\resizebox{3.3 in}{!}{

\begin{tabular}{|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l@{~}|l|}
\hline
\multirow{2}{*}{Searcher} & \multicolumn{2}{l|}{Apache} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Berkeley \\ DB C\end{tabular}} & \multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Berkeley \\ DB Java\end{tabular}} & \multicolumn{2}{l|}{LLVM} & \multicolumn{2}{l|}{SQLite} & \multicolumn{2}{l|}{X264} \\ \cline{2-13} 
                          & $\mu$         & IQR        & $\mu$                                 & IQR                                  & $\mu$                                   & IQR                                   & $\mu$       & IQR        & $\mu$      & IQR        & $\mu$       & IQR        \\ \hline
GALE                      & 870            & 0          & 0.363                                 & 0.004                                & 3139                                     & 70                                    & 202       & 3.98       & 13.1     & 0.2411     & 248      & 3.3       \\ \hline
DE                        & 840            & 0          & 0.359                                 & 0.002                                & 3139                                     & 70                                    & 200       & 0          & 13.1      & 0          & 244       & 0.003      \\ \hline
NSGA2                     & 840            & 0          & 0.354                                 & 0.005                                & 3139                                     & 70                                    & 200       & 0          & 13.1      & 0.406      & 244       & 0.05       \\ \hline
\end{tabular}}
\caption{The minimum performance scores as found by learners GALE, NSGA-II and DE, for  20 repeated
runs. Mean values are denoted $\mu$ and IQR denotes the 75th-25th percentile.}
\label{fig:external_validity}
\end{figure}



 
\subsection{RQ4}

\begin{figure*}[htbp]
\includegraphics[width=\linewidth]{Figures/compare_graph_h.eps}
\caption{Comparison between various approaches proposed in the literature ~\cite{siegmund2012predicting}, \cite{guo2013variability}, \cite{sarkar2015cost}. The figure compares \what with other techniques w.r.t. the mean error, standard deviation, and the percentage configurations used for training the model.}\label{fig:Comparison}
\end{figure*}
 
{\em Compared to the start of the art in minimal sampling for
learning runtime predictors from product lines, how good is \what with $S_1$?}\\

We compare \what with the three state of the art predictors proposed in the literature~\cite{siegmund2012predicting}, \cite{guo2013variability}, \cite{sarkar2015cost}. Siegmund et al. proposed a method called feature-wise sampling (FW) and 
 another three elaborations (PW, HO, HS) of their basic method. We use PW since relies on relatively few data to build fairly accurate predictor. Guo et al. proposed progressive random sampling methodology, which samples in steps of the number of features in the software system. The termination criteria of this technique is based on the heuristic called as $PW$ similar to the one described in  Siegmund et al.. Sarkar et al. generates a small set of configurations, with help of a heuristic called feature-frequency,  to estimate the accuracy-measure curve. The estimation is done by comparing the points to a set of predefined basis functions. This estimated curve is then used to decide the optimal number of samples to achieve a threshold error rate.  
 
 The \textcolor{blue}{\bf blue} bars of Figure \ref{fig:Comparison} show the
 mean error rate, the standard deviation of the error rate, and the mean percentage
 of total configurations used in 30 repeats of   different sampling methods
 (two from Guo et ak.\cite{guo2013variability} and one each
 from Seigmund,Sarker et al.\cite{siegmund2012predicting,sarkar2015cost} and \what).
 Note that the y-axis of that figure is a logarithmic scale so, within each plot:
 \bi
 \item Differences near the bottom  are very small differences;
 \item Differences near the top   are very large differences;
 \ei
 
As seen in the left and middle plots of
Figure 
\ref{fig:Comparison}, 
the Seigmund  method often has the highest
error rate and the highest
standard deviations on that error rate. Hence,
we cannot recommend this method. Not only
does it make mistakes most often, but due to
the size of standard deviation of this method, the  size of those
mistakes is hardest to predict. 


As to the Guo (PW) method, this   does not standout on any of
our measurements. Its error results are within 1\% of \what;
 its standard deviation are usually larger; it requires
 much more data than \what.
 
 In terms of number of measured samples require to build a model, 
 the right-hand-side plot of  Figure~\ref{fig:Comparison}  shows that
 \what uses fewest samples except for two cases:
 Guo (2N) working on BDBC and LLVM.  In both these cases, the mean error and variation on the error
 estimate is   larger than \what  (see the \textcolor{red}{{\bf RED}} bars in the left and middle plots   of Figure~\ref{fig:Comparison}). Further, in the case of BDBC, the error values
 are  $\mu=14\%, \sigma=13\%$ which are much larger
than \what's error scores of $\mu=6\%, \sigma=5\%$. 

Nor can we recommend the Sarkar  method since,
as shown on the right-hand-side of Figure~\ref{fig:Comparison}, 
this approach requires the most data and other
methods (including \what) require far less data.
While it is true that the error rate on Sarkar
is sometimes less than \what (see the left-hand-side of
Figure~\ref{fig:Comparison}), that difference is often small (1\%
to 2\%). The exception here is BDBC where Sarker has the lowest
error of any method. This exception is discussed further in the
next paragraph.
 
 


The right-hand-side of Figure \ref{fig:Comparison}   shows
the {\em percent} of configurations samples. Figure \ref{fig:measurements} shows the same values,
expressed as absolute values. Looking across that figure,
we see that many methods often require many more samples than
\what.  Using those fewest number of samples. \what has
within 1 to 2\% of the lowest standard deviation rates 
and within 1 to 2\% of lowest error rates.
The exception is Sarkar which has 5\% lower mean error
rates (in BDBC, see the left-hand-side plot of Figure \ref{fig:Comparison} ).  However, 
as shown in right-hand-side of Figure~\ref{fig:measurements}, Sarkar needs nearly three times
as many measurements than \what (191 vs 64 samples). Given
the overall reduction is error is   small (5\% difference
between Sarkar and \what in mean error), the overall
cost of tripling the data collection cost is
not justified by the very small additional benefit. 


 %Note that:
%  \bi
%     \item
    
%  \ei




\input{tex/measurement}

% \input{tex/vs2102}

% \input{tex/guo_2N}

% \input{tex/guo_pw}

% \input{tex/sarkar}

% \fig{vs2012} compares the errors found by  \what+ $S_1$:Random with those found by the minimal sampling
% methods from Siegmund et al. Note that:
% \bi
% \item
% Simple feature-wise (FW) sampling uses fewer samples that 
% other methods, but its mean error and/or standard deviation can be largest. Hence, a simple addition
% of the known performance scores for individual features is less reliable that the other methods.
% \item
% As to the other minimal sampling methods  (PW,HO,HS)  \what+$S_1$:Random always had a   lower mean
% error rate and a much lower standard deviation on the error.  
% \item In terms of number of samples required to make those predictions, \what+$S_1$:Random needed
% much less detail that PW or HO or HS
% \ei


% Guo et al. proposed progressively random sampling methodology which samples in steps of the number of features in the software system. The termination criteria of this technique is based on the heuristic called as $PW$ same as the one described in  Siegmund et al.

% \fig{guo_2n} compares the errors found by  \what+ $S_1$:Random with those found by the minimal sampling
% methods when it used $2*N$ samples. The number of configurations evaluated is very close to the  number of configurations evaluated by our method.

% \fig{guo_pw} compares the errors found by  \what+ $S_1$:Random with those found by the progressive sampling method  after allowing it to run till completion.

% Note that: 
% \bi
% \item
% Comparing the numbers in \fig{guo_2n}, our method is better than Guo et al. sampling method in terms of higher accuracy and lower standard deviation (more stability). When comparing numbers in the \fig{guo_pw} our method is very close to the mean fault rate (except for BDBJ) but significantly better in terms of stability and number of measurements especially in case SQLite.
% \item
% The results are a clear indication that random sampling in itself is not a good policy where as exploiting the underlying dimension (uses spectral clustering) can generate more accurate and stable predictors
% \ei


\begin{myshadowbox}
Hence, we answer {\bf RQ4} with ``yes'',
since our minimal sampling method yields predictions that are similar or more accurate that prior
work and rely on fewer samples.
\end{myshadowbox}


 \section{Related Work}
\label{sect:related}
 
In 2000, Shi and Maik~\cite{shi00} claimed the term ``spectral clustering'' as a reference to their normalized cuts
image
segmentation algorithm that  partitions data through a spectral (eigenvalue) analysis of the  
Laplacian representation of the similarity graph between instances in the data.

In 2003, Kamvar et al.~\cite{kamvar2003spectral}  generalized that definition saying that ``spectral learners''
were any data-mining algorithm that first replaced the raw
dimensions with those inferred from the spectrum (eigenvalues) of the affinity (a.k.a. distance)
matrix of the data, optionally adjusted via some normalization technique).

Our clustering based on first principal component splits the data on a   approximation to an eigenvector, found at each recursive level
of the data (as described in \tion{spect}). 
Hence, this  method is a ``spectral clusterer'' in the general Kamvar-sense. 
Note that
for software engineering data, we have
not found that Kamvar's normalization matrices are needed.

Regarding sampling, there are a wide range of methods know as experimental designs or designs of experiments~\cite{pukelsheim2006optimal}. They usually rely on fractional factorial designs as in the combinatorial testing community~\cite{Kuhn:2013}. 

Furthermore, there is a recent approach that learns {\em performance-influence models} for configurable software systems~\cite{SGA+15}. While this approach can handle even numeric features, it has similar sampling techniques for the Boolean features as reported in their earlier work~\cite{siegmund2012predicting}. Since we already compared to that earlier work and do not consider also numeric features, we did not compare our work to performance-influence models.
% (but perhaps if we were text mining
%on very large dimensional data, we would add in that normalization). 
 


\section{Reliability and Validity}\label{sect:construct}

{\em Reliability} refers to the consistency of the results obtained
from the research.  For example,   how well independent researchers
could reproduce the study? To increase external
reliability, this paper has taken care to either  clearly define our
algorithms or use implementations from the public domain
(SciKitLearn)~\cite{scikit-learn}. Also, all the data used in this work is available
on-line in the PROMISE code repository and all our algorithms
are on-line at github.com/ai-se/where.

{\em Validity} refers to the extent to which a piece of research actually
investigates what the researcher purports to investigate~\cite{SSA15}.
{\em Internal validity} checks if the differences found in
the treatments can be ascribed to the treatments under study. 

One internal validity issue with our experiments is the choice
of {\em training and testing} data sets discussed in 
\fig{systems}. Recall that while all our learners used the same
{\em testing} data set, our untuned learners were only given
access to {\em training}.

Another internal validity issues is {\em instrumentation}. The very low $\mu$ and $\sigma$ error values
reported in this study are so small that it is reasonable to ask they are due to some instrumentation
quirk, rather than due to using a clever sample strategy:
\begin{compactitem}
\item
Our low $\mu$ values are consistent with prior work (e.g.~\cite{sarkar2015cost} report their CART predictions;
\item
As to our low $\sigma$ values, we note that when the  error values are so close to 0\,\%, the standard
deviation of the error is ``squeezed'' between zero and those errors. Hence, we would expect that
experimental rigs
that generate error values on the order of 5\,\% \eq{err} should have $\sigma$ values of $0\le \sigma \le 5$ (e.g. like those seen in our introduction).
\end{compactitem}

Regarding SQLite, we cannot measure all possible configurations in reasonable time. Hence, we sampled only 100 configurations to compare prediction and actual performance values. We are aware that this evaluation leaves room for outliers.
Also, we are aware that measurement bias can cause false interpretations~\cite{me12d}. Since we aim at predicting performance for a special workload, we do not have to vary benchmarks.



{\em External validity}  We aimed at increasing the external validity by choosing programs from different domains with different configuration mechanisms and implemented with different programming languages. Furthermore, the programs used are deployed and used in the real world. Nevertheless, assuming the evaluations to be automatically transferable  to all configurable programs is not fair. To further strengthen the external validity, we run the model (generated by \textit{\what + $S_1$:Random} against other optimizers, such as NSGA-II and differential evolution algorithms\cite{storn1997differential}. This is to validate the fact that the model does not work only for GALE style of perturbation. In Table \ref{fig:external_validity}, we see that the models developed is valid for all searchers, as all searchers are able to find the near optimal solutions.




%That said, there exist some class of data mining papers for which
%tuning may not be required. Consider  Le Goues et al.'s 2012
%ICSE paper that used a evolutionary program to learn
%repairs to code~
%in that paper was ``can we fix any of the known bugs?''. Note
%that this criteria is a ``{\em competency}'' statement, and
%not a ``{\em better than}'' statement (the difference being that
%one is 
%``can do'' and the other is ``can do better''). For such
%competency claims, tuning is not necessary. However, as soon
%as {\em better than} enters the performance criteria then this
%becomes a race between competing methods. In such a race,
%it is unfair to hobble one competitor with poor tunings.



\section{Conclusions}

We have proposed to use a fast spectral clusterer (\what) along with three new sampling techniques which were tested on 6 real-world configurable software systems borrowed from the literature. The recommended method \textit{$S_1$:Random} achieves similar to lower fault rates while being stable, when compared our results to the state of the art methods. 
Except from the Berkeley DB system, our method performs more accurate than methods proposed by Siegmund et al.  ~\cite{siegmund2012predicting} and Guo et al. ~\cite{guo2013variability}. We achieve a similar accuracy and stability as the approach by Sarkar et al~\cite{sarkar2015cost}, while requiring far smaller number of configurations to be measured. We also show that the sampling method \textit{$S_1$:Random} can be used to make cheap (but stable) surrogates to the problems, which can be then used by various off the shelf optimizers to find the desired configuration. 

Our specific focus here was sampling data
to reason about product lines. More generally,
this paper has demonstrated the benefit of using
spectral learning for SE applications.  As argued in \tion{sample},
we recommend spectral learning  for software analytics when there are
are many possible options described by sets of attributes
that might be noisy or redundant.  



\vspace*{0.5mm}
 
 
\bibliographystyle{plain}

\balance
\bibliography{activeconfig}  
\end{document}

\section{Appendix}

\subsection{Evolutionary optimization algorithms} \label{ssec:appendix_1}
starts with an \textit{initial population} (randomly generated, based on some constraints). Since the solutions are drawn from a uniform distribution, typically, this diversity is desirable, although in some cases we might want to initialize all of the available parents to some best-known solution and proceed from there. The chosen \textit{evaluation function} must be capable of differentiating
between two individuals, i.e., it has to be able to rank one solution ahead
function, are favored to become parents for the next generation of offspring. 
A core function in evolutionary optimizers are the
\textit{reproduction} functions that generate   new solutions  probabilistically in the neighborhood of old solutions. This process continues till the a \"good enough\" solution is achieved or a hard limit on iterations are reached.  
    
    There has been extensive research in evolutionary algorithms in search-based software engineering.  For example, here is a list of search algorithms used widely in research: \textit{simulated annealing}\cite{bell2013limited, menzies2007business}; various genetic algorithms\cite{goldberg1979complexity} agumented by techniques such as
    (1)~smarter population subset selection~\cite{zit02,deb00afast};
    (2)~\textit{differential evolution} \cite{storn1997differential}
    (3)~\textit{tabu search and scatter search}\cite{nebro2008abyss, molina2007sspmo, glover1986general, beausoleil2006moss}; 
    (4)~\textit{particle swarm optimization}\cite{pan2008particle}; 
    (5)~numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods such
    as our own GALE\cite{krall2014gale,zuluaga2013active} algorithm.
    
    For this study, we applied three optimizers that are standards
    in the SE literature (NSGA-II~\cite{deb00afast} and SPEA2~\cite{zit02})
    as well as our own GALE algorithm~\cite{krall2014gale}).
    GALE uses the WHERE algorithm to find a direction of most useful
    mutation. Given leaf clusters, each with an {\em East,West} point
    then 
    GALE mutates the population of candidates in that leaf cluster towards
    which of {\em East,West} has better evaluation scores. These
    mutants become the data  that is cluster
    and mutated by WHERE in  generation $i+1$.
    
\subsection{Parameter Used}
We used the default parameters for NSGA-II and Differential Evolution (as recommended by the authors) where as the parameters used by GALE are as follows:
\begin{itemize}
\item $\mu$ = 100: population size;
\item $\omega$ = $\mu$: minimum size leaf clusters;
\item $\lambda$ = 3: premature stopping criteria (sets the maximum
allowed generations without any improvement
on any objective).
\item $\delta$ = 1: the ``accelerator'' that encourages larger
mutations;
\item $\gamma$ = 1.5: the ``brake'' that blocks excessive mutation.
\end{itemize}
\subsection{Parameter Bias} 
For this study, we did not do extensive parameter tuning:
NSGA-II and DE were run using their default
settings while GALE was run using the settings that
worked well on the first model we studied, which were
then frozen for the rest of this study. As documented
above, those parameters were:
\begin{itemize}
\item $\mu$ = 100: population size;
\item $\omega$ = $\mu$: minimum size leaf clusters;
\item $\lambda$ = 3: premature stopping criteria (sets the maximum
allowed generations without any improvement
on any objective).
\item $\delta$ = 1: the ``accelerator'' that encourages larger
mutations;
\item $\gamma$ = 1.5: the ``brake'' that blocks excessive mutation.
\end{itemize}

If this paper was arguing that these parameters were
somehow optimal, then it would be required to present
experiments defending the above settings. However, our
claim is less than that—we only aim to show that with
these settings, GALE does as well than standard searching
tools. In future work, we will explore other settings.




\end{document}